{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extraction de text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting figure pages from pages 1 to 12...\n",
      "Figure pages found: [2, 6, 17, 23, 27, 29, 32, 33, 38, 41, 44, 49, 53]\n",
      "Figure pages saved to 'output/figure_pages.txt'.\n",
      "\n",
      "Extracting full text from pages 13 to 124...\n",
      "Full text saved to 'output/extracted_text.txt'.\n",
      "\n",
      "Dividing text into 9 chapters...\n",
      "--- Chapitre 1 Introduction Snippet ---\n",
      "INTRODUCTION\n",
      "P roject Management Institute (PMI) practice standards are guides to the use of a tool, technique, or process\n",
      "identifi ed in A Guide to the Project Management Body of Knowledge ( PMBOK ® G uide – Fourth Edition) or\n",
      "other PMI standards.\n",
      "--------------------------\n",
      "\n",
      "--- Chapitre 2 Principles And Concepts Snippet ---\n",
      "PRINCIPLES AND CONCEPTS\n",
      "2.\n",
      "--------------------------\n",
      "\n",
      "--- Chapitre 3 Introduction To Project Risk Management Processes Snippet ---\n",
      "INTRODUCTION TO PROJECT RISK MANAGEMENT PROCESSES\n",
      "3.\n",
      "--------------------------\n",
      "\n",
      "--- Chapitre 4 Plan Risk Management Snippet ---\n",
      "PLAN RISK MANAGEMENT\n",
      "4.\n",
      "--------------------------\n",
      "\n",
      "--- Chapitre 5 Identify Risks Snippet ---\n",
      "IDENTIFY RISKS\n",
      "5.\n",
      "--------------------------\n",
      "\n",
      "--- Chapitre 6 Perform Qualitative Risk Analysis Snippet ---\n",
      "PERFORM QUALITATIVE RISK ANALYSIS\n",
      "6.\n",
      "--------------------------\n",
      "\n",
      "--- Chapitre 7 Perform Quantitative Risk Analysis Snippet ---\n",
      "PERFORM QUANTITATIVE RISK ANALYSIS\n",
      "7.\n",
      "--------------------------\n",
      "\n",
      "--- Chapitre 8 Plan Risk Responses Snippet ---\n",
      "PLAN RISK RESPONSES\n",
      "T he Plan Risk Responses process determines effective response actions that are appropriate to the priority\n",
      "of the individual risks and to the overall project risk.\n",
      "--------------------------\n",
      "\n",
      "--- Chapitre 9 Monitor And Control Risks Snippet ---\n",
      "MONITOR AND CONTROL RISKS\n",
      "The effectiveness of Project Risk Management depends upon the way the approved plans are carried out.\n",
      "--------------------------\n",
      "\n",
      "Chapters saved to 'output/chapters' directory.\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "import os\n",
    "\n",
    "def extract_figure_pages(pdf_path, start_page=1, end_page=12):\n",
    "    \"\"\"\n",
    "    Extracts the list of pages that contain figures from the first few pages of the PDF.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        start_page (int): Starting page number (1-indexed).\n",
    "        end_page (int): Ending page number (1-indexed).\n",
    "\n",
    "    Returns:\n",
    "        List[int]: List of page numbers that contain figures.\n",
    "    \"\"\"\n",
    "    figure_pages = []\n",
    "    figure_keywords = [\"Figure\", \"Fig.\", \"Illustration\", \"Diagram\", \"Graph\", \"Schéma\", \"Tableau\"]\n",
    "    list_of_figures_found = False\n",
    "    figure_page_pattern = re.compile(r'^(Figure|Fig\\.|Illustration|Diagram|Graph|Schéma|Tableau)\\s*\\d+', re.IGNORECASE)\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num in range(start_page - 1, end_page):\n",
    "            page = pdf.pages[page_num]\n",
    "            text = page.extract_text()\n",
    "            if not text:\n",
    "                continue\n",
    "\n",
    "            lines = text.split('\\n')\n",
    "\n",
    "            for i, line in enumerate(lines):\n",
    "                if re.search(r'List of Figures|Liste des Figures|Table des Figures|Liste des Illustrations', line, re.IGNORECASE):\n",
    "                    list_of_figures_found = True\n",
    "                    figure_start_index = i + 1\n",
    "                    break\n",
    "\n",
    "            if list_of_figures_found:\n",
    "                for line in lines[figure_start_index:]:\n",
    "                    if figure_page_pattern.match(line):\n",
    "                        page_number_match = re.search(r'\\.\\s*(\\d+)$', line)\n",
    "                        if page_number_match:\n",
    "                            page_number = int(page_number_match.group(1))\n",
    "                            figure_pages.append(page_number)\n",
    "                        else:\n",
    "                            numbers = re.findall(r'\\d+', line)\n",
    "                            if numbers:\n",
    "                                page_number = int(numbers[-1])\n",
    "                                figure_pages.append(page_number)\n",
    "                    elif re.match(r'^\\s*$', line):\n",
    "                        break\n",
    "            if list_of_figures_found:\n",
    "                break\n",
    "\n",
    "    figure_pages = sorted(list(set(figure_pages)))\n",
    "    return figure_pages\n",
    "\n",
    "def extract_chapters(pdf_path, start_page=13, end_page=124, num_chapters=9):\n",
    "    \"\"\"\n",
    "    Extracts text from the PDF and divides it into specified chapters.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        start_page (int): Starting page number for chapter extraction (1-indexed).\n",
    "        end_page (int): Ending page number for chapter extraction (1-indexed).\n",
    "        num_chapters (int): Number of chapters to divide the text into.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, str]: Dictionary with chapter titles as keys and their corresponding text as values.\n",
    "    \"\"\"\n",
    "    chapters_content = {}\n",
    "    current_chapter = None\n",
    "\n",
    "    chapter_heading_pattern = re.compile(\n",
    "        r'^(CHAPTER|CHAPITRE)\\s+(\\d+)\\s*[\\-–—−:]\\s*(.+)$',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    chapter_heading_multiline_pattern = re.compile(\n",
    "        r'^(CHAPTER|CHAPITRE)\\s+(\\d+)\\s*$',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        lines_buffer = []\n",
    "        for page_num in range(start_page - 1, end_page):\n",
    "            page = pdf.pages[page_num]\n",
    "            text = page.extract_text()\n",
    "            if not text:\n",
    "                continue\n",
    "            lines = text.split('\\n')\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "\n",
    "                chapter_match = chapter_heading_pattern.match(line)\n",
    "                if chapter_match:\n",
    "                    chapter_num = int(chapter_match.group(2))\n",
    "                    chapter_title = chapter_match.group(3).strip()\n",
    "                    if 1 <= chapter_num <= num_chapters:\n",
    "                        current_chapter = f\"Chapitre_{chapter_num}_{sanitize_filename(chapter_title)}\"\n",
    "                        if current_chapter not in chapters_content:\n",
    "                            chapters_content[current_chapter] = \"\"\n",
    "                        continue\n",
    "\n",
    "                chapter_multiline_match = chapter_heading_multiline_pattern.match(line)\n",
    "                if chapter_multiline_match:\n",
    "                    chapter_num = int(chapter_multiline_match.group(2))\n",
    "                    if 1 <= chapter_num <= num_chapters:\n",
    "                        lines_buffer.append((page_num, line))\n",
    "                        next_line_index = lines.index(line) + 1\n",
    "                        if next_line_index < len(lines):\n",
    "                            next_line = lines[next_line_index].strip()\n",
    "                            if next_line:\n",
    "                                chapter_title = next_line\n",
    "                                current_chapter = f\"Chapitre_{chapter_num}_{sanitize_filename(chapter_title)}\"\n",
    "                                if current_chapter not in chapters_content:\n",
    "                                    chapters_content[current_chapter] = \"\"\n",
    "                                continue\n",
    "\n",
    "                if current_chapter:\n",
    "                    if re.match(r'^\\d+$', line):\n",
    "                        continue\n",
    "                    chapters_content[current_chapter] += line + \"\\n\"\n",
    "\n",
    "    return chapters_content\n",
    "\n",
    "def sanitize_filename(name):\n",
    "    \"\"\"\n",
    "    Sanitizes a string to be used as a filename by removing or replacing invalid characters.\n",
    "\n",
    "    Args:\n",
    "        name (str): The string to sanitize.\n",
    "\n",
    "    Returns:\n",
    "        str: A sanitized string suitable for filenames.\n",
    "    \"\"\"\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", name.lower().replace(\" \", \"_\"))\n",
    "\n",
    "def extract_first_sentence(text):\n",
    "    \"\"\"\n",
    "    Extracts the first sentence from a block of text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to extract the sentence from.\n",
    "\n",
    "    Returns:\n",
    "        str: The first sentence.\n",
    "    \"\"\"\n",
    "    sentence_endings = re.compile(r'[.!?]')\n",
    "    match = sentence_endings.search(text)\n",
    "    if match:\n",
    "        end = match.end()\n",
    "        return text[:end].strip()\n",
    "    else:\n",
    "        return text[:100].strip()\n",
    "\n",
    "def save_chapters(chapters, output_dir=\"output/chapters\"):\n",
    "    \"\"\"\n",
    "    Saves each chapter's content to a separate text file and prints a snippet from each chapter.\n",
    "\n",
    "    Args:\n",
    "        chapters (Dict[str, str]): Dictionary with chapter titles and their text.\n",
    "        output_dir (str): Directory where chapter files will be saved.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for chapter, content in chapters.items():\n",
    "        filename = f\"{chapter}.txt\"\n",
    "        file_path = os.path.join(output_dir, filename)\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(content)\n",
    "        first_sentence = extract_first_sentence(content)\n",
    "        print(f\"--- {chapter.replace('_', ' ').title()} Snippet ---\")\n",
    "        print(first_sentence)\n",
    "        print(\"--------------------------\\n\")\n",
    "    print(f\"Chapters saved to '{output_dir}' directory.\")\n",
    "\n",
    "def save_figure_pages(figure_pages, output_file=\"output/figure_pages.txt\"):\n",
    "    \"\"\"\n",
    "    Saves the list of figure pages to a text file.\n",
    "\n",
    "    Args:\n",
    "        figure_pages (List[int]): List of page numbers containing figures.\n",
    "        output_file (str): Path to the output text file.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for page in figure_pages:\n",
    "            f.write(f\"Page {page}\\n\")\n",
    "    print(f\"Figure pages saved to '{output_file}'.\")\n",
    "\n",
    "def save_full_text(text, output_file=\"output/extracted_text.txt\"):\n",
    "    \"\"\"\n",
    "    Saves the extracted full text to a text file.\n",
    "\n",
    "    Args:\n",
    "        text (str): The full extracted text.\n",
    "        output_file (str): Path to the output text file.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "    print(f\"Full text saved to '{output_file}'.\")\n",
    "\n",
    "def extract_full_text(pdf_path, start_page=13, end_page=124):\n",
    "    \"\"\"\n",
    "    Extracts the full text from specified pages.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        start_page (int): Starting page number (1-indexed).\n",
    "        end_page (int): Ending page number (1-indexed).\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted full text.\n",
    "    \"\"\"\n",
    "    full_text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page_num in range(start_page - 1, end_page):\n",
    "            if page_num < 0 or page_num >= len(pdf.pages):\n",
    "                continue\n",
    "            page = pdf.pages[page_num]\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                full_text += text + \"\\n\"\n",
    "    return full_text\n",
    "\n",
    "def main():\n",
    "    pdf_path = \"practice-standard-project-risk-management.pdf\"\n",
    "\n",
    "    if not os.path.isfile(pdf_path):\n",
    "        print(f\"Error: PDF file '{pdf_path}' not found.\")\n",
    "        return\n",
    "\n",
    "    print(\"Extracting figure pages from pages 1 to 12...\")\n",
    "    figure_pages = extract_figure_pages(pdf_path, start_page=1, end_page=12)\n",
    "    if not figure_pages:\n",
    "        print(\"No figure pages found.\")\n",
    "    else:\n",
    "        print(f\"Figure pages found: {figure_pages}\")\n",
    "        save_figure_pages(figure_pages, output_file=\"output/figure_pages.txt\")\n",
    "\n",
    "    print(\"\\nExtracting full text from pages 13 to 124...\")\n",
    "    full_text = extract_full_text(pdf_path, start_page=13, end_page=124)\n",
    "    save_full_text(full_text, output_file=\"output/extracted_text.txt\")\n",
    "\n",
    "    print(\"\\nDividing text into 9 chapters...\")\n",
    "    chapters = extract_chapters(pdf_path, start_page=13, end_page=124, num_chapters=9)\n",
    "    save_chapters(chapters, output_dir=\"output/chapters\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning chapters...\n",
      "Cleaned and saved Chapitre_1_introduction.txt to output/cleaned_chapters\n",
      "Cleaned and saved Chapitre_2_principles_and_concepts.txt to output/cleaned_chapters\n",
      "Cleaned and saved Chapitre_3_introduction_to_project_risk_management_processes.txt to output/cleaned_chapters\n",
      "Cleaned and saved Chapitre_4_plan_risk_management.txt to output/cleaned_chapters\n",
      "Cleaned and saved Chapitre_5_identify_risks.txt to output/cleaned_chapters\n",
      "Cleaned and saved Chapitre_6_perform_qualitative_risk_analysis.txt to output/cleaned_chapters\n",
      "Cleaned and saved Chapitre_7_perform_quantitative_risk_analysis.txt to output/cleaned_chapters\n",
      "Cleaned and saved Chapitre_8_plan_risk_responses.txt to output/cleaned_chapters\n",
      "Cleaned and saved Chapitre_9_monitor_and_control_risks.txt to output/cleaned_chapters\n",
      "All chapters cleaned.\n",
      "\n",
      "Saving final chapters...\n",
      "Saved final chapter to output/final_chapters\\chapitre_1_introduction.txt\n",
      "Saved final chapter to output/final_chapters\\chapitre_2_principles_and_concepts.txt\n",
      "Saved final chapter to output/final_chapters\\chapitre_3_introduction_to_project_risk_management_processes.txt\n",
      "Saved final chapter to output/final_chapters\\chapitre_4_plan_risk_management.txt\n",
      "Saved final chapter to output/final_chapters\\chapitre_5_identify_risks.txt\n",
      "Saved final chapter to output/final_chapters\\chapitre_6_perform_qualitative_risk_analysis.txt\n",
      "Saved final chapter to output/final_chapters\\chapitre_7_perform_quantitative_risk_analysis.txt\n",
      "Saved final chapter to output/final_chapters\\chapitre_8_plan_risk_responses.txt\n",
      "Saved final chapter to output/final_chapters\\chapitre_9_monitor_and_control_risks.txt\n",
      "All final chapters saved.\n",
      "\n",
      "Processing completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "import pdfplumber\n",
    "\n",
    "def sanitize_filename(name):\n",
    "    \"\"\"\n",
    "    Sanitizes a string to be used as a filename by removing or replacing invalid characters.\n",
    "\n",
    "    Args:\n",
    "        name (str): The string to sanitize.\n",
    "\n",
    "    Returns:\n",
    "        str: A sanitized string suitable for filenames.\n",
    "    \"\"\"\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", name.lower().replace(\" \", \"_\"))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the input text by removing standalone numbers and normalizing whitespace.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to clean.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned text.\n",
    "    \"\"\"\n",
    "    # Remove all standalone numbers\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Strip leading and trailing spaces\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def get_chapter_name_from_page(pdf, page_num):\n",
    "    \"\"\"\n",
    "    Extracts the chapter name from the top of the specified page.\n",
    "\n",
    "    Args:\n",
    "        pdf (pdfplumber.PDF): The opened PDF object.\n",
    "        page_num (int): The 1-indexed page number.\n",
    "\n",
    "    Returns:\n",
    "        str: Sanitized chapter name if found, else None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        page = pdf.pages[page_num - 1]  # 0-indexed\n",
    "        text = page.extract_text()\n",
    "        if not text:\n",
    "            return None\n",
    "        lines = text.split('\\n')\n",
    "        for line in lines[:5]:  # Assume chapter name is within the first 5 lines\n",
    "            line = line.strip()\n",
    "            # Match chapter headings similar to extract_chapters function\n",
    "            chapter_heading_pattern = re.compile(\n",
    "                r'^(CHAPTER|CHAPITRE)\\s+(\\d+)\\s*[\\-–—−:]\\s*(.+)$',\n",
    "                re.IGNORECASE\n",
    "            )\n",
    "            chapter_multiline_pattern = re.compile(\n",
    "                r'^(CHAPTER|CHAPITRE)\\s+(\\d+)\\s*$',\n",
    "                re.IGNORECASE\n",
    "            )\n",
    "            match = chapter_heading_pattern.match(line)\n",
    "            if match:\n",
    "                chapter_num = int(match.group(2))\n",
    "                chapter_title = match.group(3).strip()\n",
    "                sanitized_title = sanitize_filename(chapter_title)\n",
    "                chapter_name = f\"chapitre_{chapter_num}_{sanitized_title}\".lower()\n",
    "                return chapter_name\n",
    "            else:\n",
    "                match = chapter_multiline_pattern.match(line)\n",
    "                if match:\n",
    "                    chapter_num = int(match.group(2))\n",
    "                    # Assume next line is the title\n",
    "                    current_index = lines.index(line)\n",
    "                    if current_index + 1 < len(lines):\n",
    "                        next_line = lines[current_index + 1].strip()\n",
    "                        if next_line:\n",
    "                            chapter_title = next_line\n",
    "                            sanitized_title = sanitize_filename(chapter_title)\n",
    "                            chapter_name = f\"chapitre_{chapter_num}_{sanitized_title}\".lower()\n",
    "                            return chapter_name\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting chapter name from page {page_num}: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # Define paths\n",
    "    pdf_path = \"content/practice-standard-project-risk-management.pdf\"\n",
    "    chapters_dir = \"output/chapters\"\n",
    "    cleaned_chapters_dir = \"output/cleaned_chapters\"\n",
    "    final_chapters_dir = \"output/final_chapters\"\n",
    "\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(cleaned_chapters_dir, exist_ok=True)\n",
    "    os.makedirs(final_chapters_dir, exist_ok=True)\n",
    "\n",
    "    # Step 1: Clean the text for each chapter and save\n",
    "    print(\"Cleaning chapters...\")\n",
    "    chapters = {}\n",
    "    for filename in os.listdir(chapters_dir):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            chapter_path = os.path.join(chapters_dir, filename)\n",
    "            with open(chapter_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "            cleaned = clean_text(text)\n",
    "            # Save cleaned text\n",
    "            cleaned_filename = filename.replace(\"chapitre_\", \"cleaned_chapitre_\")\n",
    "            cleaned_path = os.path.join(cleaned_chapters_dir, cleaned_filename)\n",
    "            with open(cleaned_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(cleaned)\n",
    "            # Store in dict for later use\n",
    "            chapter_key = filename.replace(\".txt\", \"\").lower()\n",
    "            chapters[chapter_key] = cleaned\n",
    "            print(f\"Cleaned and saved {filename} to {cleaned_chapters_dir}\")\n",
    "    print(\"All chapters cleaned.\\n\")\n",
    "\n",
    "    # Step 2: Save the combined text for each chapter\n",
    "    print(\"Saving final chapters...\")\n",
    "    for chapter, text in chapters.items():\n",
    "        final_path = os.path.join(final_chapters_dir, f\"{chapter}.txt\")\n",
    "        with open(final_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(text)\n",
    "        print(f\"Saved final chapter to {final_path}\")\n",
    "    print(\"All final chapters saved.\\n\")\n",
    "\n",
    "    print(\"Processing completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 'chapitre_1_introduction.txt':\n",
      " - First 5 stop words removed: are, to, the, of, a\n",
      "\n",
      "Processed 'chapitre_2_principles_and_concepts.txt':\n",
      " - First 5 stop words removed: and, t, his, the, to\n",
      "\n",
      "Processed 'chapitre_3_introduction_to_project_risk_management_processes.txt':\n",
      " - First 5 stop words removed: to, and, all, are, is\n",
      "\n",
      "Processed 'chapitre_4_plan_risk_management.txt':\n",
      " - First 5 stop words removed: and, of, the, t, he\n",
      "\n",
      "Processed 'chapitre_5_identify_risks.txt':\n",
      " - First 5 stop words removed: and, of, the, a, be\n",
      "\n",
      "Processed 'chapitre_6_perform_qualitative_risk_analysis.txt':\n",
      " - First 5 stop words removed: and, of, the, t, he\n",
      "\n",
      "Processed 'chapitre_7_perform_quantitative_risk_analysis.txt':\n",
      " - First 5 stop words removed: and, of, the, the, a\n",
      "\n",
      "Processed 'chapitre_8_plan_risk_responses.txt':\n",
      " - First 5 stop words removed: t, he, that, are, to\n",
      "\n",
      "Processed 'chapitre_9_monitor_and_control_risks.txt':\n",
      " - First 5 stop words removed: and, the, of, the, the\n",
      "\n",
      "All chapters have been processed and cleaned.\n",
      "\n",
      "Summary of stop words removed for each chapter:\n",
      " - Chapitre 1 Introduction: are, to, the, of, a\n",
      " - Chapitre 2 Principles And Concepts: and, t, his, the, to\n",
      " - Chapitre 3 Introduction To Project Risk Management Processes: to, and, all, are, is\n",
      " - Chapitre 4 Plan Risk Management: and, of, the, t, he\n",
      " - Chapitre 5 Identify Risks: and, of, the, a, be\n",
      " - Chapitre 6 Perform Qualitative Risk Analysis: and, of, the, t, he\n",
      " - Chapitre 7 Perform Quantitative Risk Analysis: and, of, the, the, a\n",
      " - Chapitre 8 Plan Risk Responses: t, he, that, are, to\n",
      " - Chapitre 9 Monitor And Control Risks: and, the, of, the, the\n",
      "\n",
      "Cleaned chapters saved to 'output/cleaned_stopwords_chapters' directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stop_words(text, stop_words, max_removed=5):\n",
    "    \"\"\"\n",
    "    Removes stop words from the text and returns the cleaned text along with the first few stop words removed.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to process.\n",
    "        stop_words (set): A set of stop words to remove.\n",
    "        max_removed (int): Maximum number of stop words to record.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, List[str]]: Cleaned text and list of first stop words removed.\n",
    "    \"\"\"\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    cleaned_words = []\n",
    "    removed_stop_words = []\n",
    "    for word in words:\n",
    "        word_lower = word.lower()\n",
    "        if word_lower in stop_words:\n",
    "            if len(removed_stop_words) < max_removed:\n",
    "                removed_stop_words.append(word_lower)\n",
    "            continue\n",
    "        cleaned_words.append(word)\n",
    "    cleaned_text = ' '.join(cleaned_words)\n",
    "    return cleaned_text, removed_stop_words\n",
    "\n",
    "def main():\n",
    "    # Define paths\n",
    "    final_chapters_dir = \"output/final_chapters\"  # Directory with final chapters\n",
    "    cleaned_stopwords_chapters_dir = \"output/cleaned_stopwords_chapters\"  # Directory to save cleaned chapters\n",
    "\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(cleaned_stopwords_chapters_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize stop words (English and French)\n",
    "    try:\n",
    "        stop_words_en = set(stopwords.words('english'))\n",
    "    except LookupError:\n",
    "        nltk.download('stopwords')\n",
    "        stop_words_en = set(stopwords.words('english'))\n",
    "        \n",
    "    try:\n",
    "        stop_words_fr = set(stopwords.words('french'))\n",
    "    except LookupError:\n",
    "        nltk.download('stopwords')\n",
    "        stop_words_fr = set(stopwords.words('french'))\n",
    "        \n",
    "    stop_words = stop_words_en.union(stop_words_fr)\n",
    "\n",
    "    # Initialize a list to hold mapping information\n",
    "    mapping_info = []\n",
    "\n",
    "    # Iterate through each final chapter file\n",
    "    for filename in os.listdir(final_chapters_dir):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            chapter_path = os.path.join(final_chapters_dir, filename)\n",
    "            with open(chapter_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "\n",
    "            # Remove stop words from the text and collect first 5 removed\n",
    "            cleaned_text, first_5_stop_words = remove_stop_words(text, stop_words, max_removed=5)\n",
    "\n",
    "            # Save the cleaned chapter\n",
    "            cleaned_filename = filename.replace(\"chapitre_\", \"cleaned_chapitre_\")\n",
    "            cleaned_path = os.path.join(cleaned_stopwords_chapters_dir, cleaned_filename)\n",
    "            with open(cleaned_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(cleaned_text)\n",
    "\n",
    "            # Extract chapter name without extension for readability\n",
    "            chapter_name = filename.replace(\".txt\", \"\").replace(\"_\", \" \").title()\n",
    "\n",
    "            # Record the mapping information\n",
    "            mapping_info.append((filename, chapter_name, first_5_stop_words))\n",
    "\n",
    "            # Print the first 5 stop words removed\n",
    "            print(f\"Processed '{filename}':\")\n",
    "            if first_5_stop_words:\n",
    "                print(f\" - First 5 stop words removed: {', '.join(first_5_stop_words)}\")\n",
    "            else:\n",
    "                print(\" - No stop words were removed.\")\n",
    "            print()\n",
    "\n",
    "    print(\"All chapters have been processed and cleaned.\")\n",
    "    print(\"\\nSummary of stop words removed for each chapter:\")\n",
    "    for desc_file, chap_name, stop_words_removed in mapping_info:\n",
    "        if stop_words_removed:\n",
    "            print(f\" - {chap_name}: {', '.join(stop_words_removed)}\")\n",
    "        else:\n",
    "            print(f\" - {chap_name}: No stop words removed.\")\n",
    "\n",
    "    print(\"\\nCleaned chapters saved to 'output/cleaned_stopwords_chapters' directory.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 'cleaned_chapitre_1_introduction.txt':\n",
      " - First 5 lemmatization examples:\n",
      "    * standards -> standard\n",
      "    * guides -> guide\n",
      "    * targeted -> target\n",
      "    * audiences -> audience\n",
      "    * projects -> project\n",
      "\n",
      "Processed 'cleaned_chapitre_2_principles_and_concepts.txt':\n",
      " - First 5 lemmatization examples:\n",
      "    * introduces -> introduce\n",
      "    * ideas -> idea\n",
      "    * required -> require\n",
      "    * projects -> project\n",
      "    * following -> follow\n",
      "\n",
      "Processed 'cleaned_chapitre_3_introduction_to_project_risk_management_processes.txt':\n",
      " - First 5 lemmatization examples:\n",
      "    * projects -> project\n",
      "    * undertakings -> undertaking\n",
      "    * based -> base\n",
      "    * assumptions -> assumption\n",
      "    * constraints -> constraint\n",
      "\n",
      "Processed 'cleaned_chapitre_4_plan_risk_management.txt':\n",
      " - First 5 lemmatization examples:\n",
      "    * objectives -> objective\n",
      "    * processes -> process\n",
      "    * executed -> execute\n",
      "    * activities -> activity\n",
      "    * requires -> require\n",
      "\n",
      "Processed 'cleaned_chapitre_5_identify_risks.txt':\n",
      " - First 5 lemmatization examples:\n",
      "    * Objectives -> objective\n",
      "    * managed -> manage\n",
      "    * completed -> complete\n",
      "    * aims -> aim\n",
      "    * risks -> risk\n",
      "\n",
      "Processed 'cleaned_chapitre_6_perform_qualitative_risk_analysis.txt':\n",
      " - First 5 lemmatization examples:\n",
      "    * assesses -> assess\n",
      "    * evaluates -> evaluate\n",
      "    * characteristics -> characteristic\n",
      "    * risks -> risk\n",
      "    * prioritizes -> prioritize\n",
      "\n",
      "Processed 'cleaned_chapitre_7_perform_quantitative_risk_analysis.txt':\n",
      " - First 5 lemmatization examples:\n",
      "    * provides -> provide\n",
      "    * objectives -> objective\n",
      "    * based -> base\n",
      "    * plans -> plan\n",
      "    * considering -> consider\n",
      "\n",
      "Processed 'cleaned_chapitre_8_plan_risk_responses.txt':\n",
      " - First 5 lemmatization examples:\n",
      "    * RESPONSES -> response\n",
      "    * determines -> determine\n",
      "    * actions -> action\n",
      "    * risks -> risk\n",
      "    * takes -> take\n",
      "\n",
      "Processed 'cleaned_chapitre_9_monitor_and_control_risks.txt':\n",
      " - First 5 lemmatization examples:\n",
      "    * depends -> depend\n",
      "    * approved -> approve\n",
      "    * plans -> plan\n",
      "    * carried -> carry\n",
      "    * executed -> execute\n",
      "\n",
      "All chapters have been lemmatized and saved.\n",
      "\n",
      "Summary of lemmatization applied to each chapter:\n",
      " - 1 Introduction: standards -> standard, guides -> guide, targeted -> target, audiences -> audience, projects -> project\n",
      " - 2 Principles And Concepts: introduces -> introduce, ideas -> idea, required -> require, projects -> project, following -> follow\n",
      " - 3 Introduction To Project Risk Management Processes: projects -> project, undertakings -> undertaking, based -> base, assumptions -> assumption, constraints -> constraint\n",
      " - 4 Plan Risk Management: objectives -> objective, processes -> process, executed -> execute, activities -> activity, requires -> require\n",
      " - 5 Identify Risks: Objectives -> objective, managed -> manage, completed -> complete, aims -> aim, risks -> risk\n",
      " - 6 Perform Qualitative Risk Analysis: assesses -> assess, evaluates -> evaluate, characteristics -> characteristic, risks -> risk, prioritizes -> prioritize\n",
      " - 7 Perform Quantitative Risk Analysis: provides -> provide, objectives -> objective, based -> base, plans -> plan, considering -> consider\n",
      " - 8 Plan Risk Responses: RESPONSES -> response, determines -> determine, actions -> action, risks -> risk, takes -> take\n",
      " - 9 Monitor And Control Risks: depends -> depend, approved -> approve, plans -> plan, carried -> carry, executed -> execute\n",
      "\n",
      "Lemmatized chapters saved to 'output/lemmatized_chapters' directory.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import spacy\n",
    "from langdetect import detect, LangDetectException\n",
    "from spacy.cli import download as spacy_download\n",
    "\n",
    "# Install spaCy models if they are not already installed\n",
    "def install_spacy_models():\n",
    "    \"\"\"\n",
    "    Installs the required spaCy language models if they are not already installed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        spacy.load('en_core_web_sm')\n",
    "    except OSError:\n",
    "        print(\"Downloading 'en_core_web_sm' model...\")\n",
    "        spacy_download('en_core_web_sm')\n",
    "    \n",
    "    try:\n",
    "        spacy.load('fr_core_news_sm')\n",
    "    except OSError:\n",
    "        print(\"Downloading 'fr_core_news_sm' model...\")\n",
    "        spacy_download('fr_core_news_sm')\n",
    "\n",
    "# Detect the language of the text\n",
    "def detect_language(text):\n",
    "    \"\"\"\n",
    "    Detects the language of the given text using langdetect.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to detect language for.\n",
    "\n",
    "    Returns:\n",
    "        str: Detected language code ('en' for English, 'fr' for French).\n",
    "             Returns 'unknown' if detection fails or the language is neither English nor French.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        return lang if lang in ['en', 'fr'] else 'unknown'\n",
    "    except LangDetectException:\n",
    "        return 'unknown'\n",
    "\n",
    "# Lemmatize the text using spaCy\n",
    "def lemmatize_text(text, nlp):\n",
    "    \"\"\"\n",
    "    Lemmatizes the given text using the provided spaCy NLP model.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to lemmatize.\n",
    "        nlp (spacy.lang.*.Language): The spaCy NLP model.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, List[Tuple[str, str]]]: The lemmatized text and a list of tuples containing original and lemmatized words.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    lemmatized_words = []\n",
    "    cleaned_tokens = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.is_alpha:  # Check if the token is alphabetic\n",
    "            lemma = token.lemma_\n",
    "            if lemma != token.text.lower():\n",
    "                lemmatized_words.append((token.text, lemma))  # Track lemmatization changes\n",
    "            cleaned_tokens.append(lemma)\n",
    "        else:\n",
    "            cleaned_tokens.append(token.text)\n",
    "    \n",
    "    lemmatized_text = ' '.join(cleaned_tokens)\n",
    "    return lemmatized_text, lemmatized_words\n",
    "\n",
    "# Split the chapter text into main text and figure descriptions\n",
    "def split_chapter_text(text):\n",
    "    \"\"\"\n",
    "    Splits the chapter text into main text and figure descriptions.\n",
    "\n",
    "    Args:\n",
    "        text (str): The complete chapter text.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, List[str]]: A tuple containing the main text and a list of figure descriptions.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'\\n\\nFigure Description from page \\d+:\\n', re.IGNORECASE)\n",
    "    parts = pattern.split(text)\n",
    "    main_text = parts[0].strip()\n",
    "    figure_descriptions = parts[1:]  # The remaining parts are figure descriptions\n",
    "    return main_text, figure_descriptions\n",
    "\n",
    "# Main function to process chapters\n",
    "def main():\n",
    "    # Install spaCy models if not already installed\n",
    "    install_spacy_models()\n",
    "    \n",
    "    # Load spaCy models\n",
    "    nlp_en = spacy.load('en_core_web_sm')\n",
    "    nlp_fr = spacy.load('fr_core_news_sm')\n",
    "    \n",
    "    # Define paths\n",
    "    cleaned_stopwords_chapters_dir = \"output/cleaned_stopwords_chapters\"  # Directory with stop words removed\n",
    "    lemmatized_chapters_dir = \"output/lemmatized_chapters\"  # Directory to save lemmatized chapters\n",
    "    \n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(lemmatized_chapters_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize a list to hold mapping information\n",
    "    mapping_info = []\n",
    "    \n",
    "    # Iterate through each cleaned chapter file\n",
    "    for filename in os.listdir(cleaned_stopwords_chapters_dir):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            chapter_path = os.path.join(cleaned_stopwords_chapters_dir, filename)\n",
    "            with open(chapter_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "    \n",
    "            # Split the text into main text and figure descriptions\n",
    "            main_text, figure_descriptions = split_chapter_text(text)\n",
    "    \n",
    "            # Detect language of the main text\n",
    "            language = detect_language(main_text)\n",
    "            nlp = nlp_en if language == 'en' else nlp_fr if language == 'fr' else None\n",
    "            \n",
    "            if nlp is None:\n",
    "                print(f\"Could not detect language for '{filename}'. Skipping lemmatization.\")\n",
    "                continue\n",
    "    \n",
    "            # Lemmatize the main text and record word transformations\n",
    "            lemmatized_main_text, lemmatized_words = lemmatize_text(main_text, nlp)\n",
    "    \n",
    "            # Reassemble the lemmatized text with figure descriptions\n",
    "            lemmatized_text = lemmatized_main_text\n",
    "            for desc in figure_descriptions:\n",
    "                lemmatized_text += f\"\\n\\nFigure Description:\\n{desc.strip()}\"\n",
    "    \n",
    "            # Save the lemmatized chapter\n",
    "            lemmatized_filename = filename.replace(\"cleaned_chapitre_\", \"lemmatized_chapitre_\")\n",
    "            lemmatized_path = os.path.join(lemmatized_chapters_dir, lemmatized_filename)\n",
    "            with open(lemmatized_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(lemmatized_text)\n",
    "    \n",
    "            # Extract chapter name without extension for readability\n",
    "            chapter_name = filename.replace(\".txt\", \"\").replace(\"cleaned_chapitre_\", \"\").replace(\"_\", \" \").title()\n",
    "    \n",
    "            # Select first five unique lemmatization examples\n",
    "            unique_lemmatizations = []\n",
    "            seen = set()\n",
    "            for original, lemma in lemmatized_words:\n",
    "                if original.lower() != lemma.lower() and original.lower() not in seen:\n",
    "                    unique_lemmatizations.append((original, lemma))\n",
    "                    seen.add(original.lower())\n",
    "                if len(unique_lemmatizations) >= 5:\n",
    "                    break\n",
    "    \n",
    "            # Record the mapping information\n",
    "            mapping_info.append((filename, chapter_name, unique_lemmatizations))\n",
    "    \n",
    "            # Print the first five lemmatization examples\n",
    "            print(f\"Processed '{filename}':\")\n",
    "            if unique_lemmatizations:\n",
    "                print(\" - First 5 lemmatization examples:\")\n",
    "                for orig, lem in unique_lemmatizations:\n",
    "                    print(f\"    * {orig} -> {lem}\")\n",
    "            else:\n",
    "                print(\" - No lemmatization changes were made.\")\n",
    "            print()\n",
    "    \n",
    "    print(\"All chapters have been lemmatized and saved.\")\n",
    "    print(\"\\nSummary of lemmatization applied to each chapter:\")\n",
    "    for desc_file, chap_name, lemmatizations in mapping_info:\n",
    "        if lemmatizations:\n",
    "            examples = ', '.join([f\"{orig} -> {lem}\" for orig, lem in lemmatizations])\n",
    "            print(f\" - {chap_name}: {examples}\")\n",
    "        else:\n",
    "            print(f\" - {chap_name}: No lemmatization changes.\")\n",
    "    \n",
    "    print(\"\\nLemmatized chapters saved to 'output/lemmatized_chapters' directory.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PostTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langdetect\n",
      "  Using cached langdetect-1.0.9.tar.gz (981 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: six in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langdetect) (1.16.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (pyproject.toml): started\n",
      "  Building wheel for langdetect (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993251 sha256=d9d4e8141b8f16eec528800ab7d53f7bc4fd898a85f0c6f0c050ac4cf433fd56\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\c1\\67\\88\\e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Lemmatized Chapitre 1 Introduction ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS Tag</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>management</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>project</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>project</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>risk</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>process</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word POS Tag  Frequency\n",
       "0  management   PROPN         83\n",
       "1     project   PROPN         80\n",
       "2     project    NOUN         76\n",
       "3        risk   PROPN         72\n",
       "4     process    NOUN         37"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Lemmatized Chapitre 2 Principles And Concepts ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS Tag</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>risk</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>project</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>project</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>management</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>risk</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word POS Tag  Frequency\n",
       "0        risk    NOUN         69\n",
       "1     project    NOUN         61\n",
       "2     project   PROPN         36\n",
       "3  management   PROPN         32\n",
       "4        risk   PROPN         29"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Lemmatized Chapitre 3 Introduction To Project Risk Management Processes ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS Tag</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>risk</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>project</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>project</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>management</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>risk</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word POS Tag  Frequency\n",
       "0        risk    NOUN         63\n",
       "1     project    NOUN         49\n",
       "2     project   PROPN         47\n",
       "3  management   PROPN         44\n",
       "4        risk   PROPN         43"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Lemmatized Chapitre 4 Plan Risk Management ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS Tag</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>risk</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>management</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>project</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>management</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>project</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word POS Tag  Frequency\n",
       "0        risk    NOUN         84\n",
       "1  management    NOUN         66\n",
       "2     project    NOUN         57\n",
       "3  management   PROPN         43\n",
       "4     project   PROPN         32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Lemmatized Chapitre 5 Identify Risks ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS Tag</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>risk</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>project</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>identifi</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>process</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>risks</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word POS Tag  Frequency\n",
       "0      risk    NOUN         69\n",
       "1   project    NOUN         32\n",
       "2  identifi   PROPN         29\n",
       "3   process    NOUN         17\n",
       "4     risks   PROPN         16"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Lemmatized Chapitre 6 Perform Qualitative Risk Analysis ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS Tag</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>risk</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>project</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>risk</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>may</td>\n",
       "      <td>AUX</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>analysis</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word POS Tag  Frequency\n",
       "0      risk    NOUN         85\n",
       "1   project    NOUN         31\n",
       "2      risk   PROPN         19\n",
       "3       may     AUX         18\n",
       "4  analysis    NOUN         16"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Lemmatized Chapitre 7 Perform Quantitative Risk Analysis ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS Tag</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>risk</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>project</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>quantitative</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>analysis</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cost</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word POS Tag  Frequency\n",
       "0          risk    NOUN         87\n",
       "1       project    NOUN         60\n",
       "2  quantitative     ADJ         32\n",
       "3      analysis    NOUN         29\n",
       "4          cost    NOUN         21"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Lemmatized Chapitre 8 Plan Risk Responses ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS Tag</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>risk</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>response</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>project</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>plan</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>action</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word POS Tag  Frequency\n",
       "0      risk    NOUN        116\n",
       "1  response    NOUN         67\n",
       "2   project    NOUN         45\n",
       "3      plan    NOUN         29\n",
       "4    action    NOUN         28"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Lemmatized Chapitre 9 Monitor And Control Risks ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS Tag</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>risk</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>project</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pmp</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>project</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>management</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word POS Tag  Frequency\n",
       "0        risk    NOUN        389\n",
       "1     project    NOUN        230\n",
       "2         pmp   PROPN        186\n",
       "3     project   PROPN        179\n",
       "4  management   PROPN        167"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "All chapters have been POS tagged and tables have been saved.\n",
      "\n",
      "Summary of top 5 words for each chapter:\n",
      " - Lemmatized Chapitre 1 Introduction:\n",
      "    * management (PROPN) - 83 times\n",
      "    * project (PROPN) - 80 times\n",
      "    * project (NOUN) - 76 times\n",
      "    * risk (PROPN) - 72 times\n",
      "    * process (NOUN) - 37 times\n",
      "\n",
      " - Lemmatized Chapitre 2 Principles And Concepts:\n",
      "    * risk (NOUN) - 69 times\n",
      "    * project (NOUN) - 61 times\n",
      "    * project (PROPN) - 36 times\n",
      "    * management (PROPN) - 32 times\n",
      "    * risk (PROPN) - 29 times\n",
      "\n",
      " - Lemmatized Chapitre 3 Introduction To Project Risk Management Processes:\n",
      "    * risk (NOUN) - 63 times\n",
      "    * project (NOUN) - 49 times\n",
      "    * project (PROPN) - 47 times\n",
      "    * management (PROPN) - 44 times\n",
      "    * risk (PROPN) - 43 times\n",
      "\n",
      " - Lemmatized Chapitre 4 Plan Risk Management:\n",
      "    * risk (NOUN) - 84 times\n",
      "    * management (NOUN) - 66 times\n",
      "    * project (NOUN) - 57 times\n",
      "    * management (PROPN) - 43 times\n",
      "    * project (PROPN) - 32 times\n",
      "\n",
      " - Lemmatized Chapitre 5 Identify Risks:\n",
      "    * risk (NOUN) - 69 times\n",
      "    * project (NOUN) - 32 times\n",
      "    * identifi (PROPN) - 29 times\n",
      "    * process (NOUN) - 17 times\n",
      "    * risks (PROPN) - 16 times\n",
      "\n",
      " - Lemmatized Chapitre 6 Perform Qualitative Risk Analysis:\n",
      "    * risk (NOUN) - 85 times\n",
      "    * project (NOUN) - 31 times\n",
      "    * risk (PROPN) - 19 times\n",
      "    * may (AUX) - 18 times\n",
      "    * analysis (NOUN) - 16 times\n",
      "\n",
      " - Lemmatized Chapitre 7 Perform Quantitative Risk Analysis:\n",
      "    * risk (NOUN) - 87 times\n",
      "    * project (NOUN) - 60 times\n",
      "    * quantitative (ADJ) - 32 times\n",
      "    * analysis (NOUN) - 29 times\n",
      "    * cost (NOUN) - 21 times\n",
      "\n",
      " - Lemmatized Chapitre 8 Plan Risk Responses:\n",
      "    * risk (NOUN) - 116 times\n",
      "    * response (NOUN) - 67 times\n",
      "    * project (NOUN) - 45 times\n",
      "    * plan (NOUN) - 29 times\n",
      "    * action (NOUN) - 28 times\n",
      "\n",
      " - Lemmatized Chapitre 9 Monitor And Control Risks:\n",
      "    * risk (NOUN) - 389 times\n",
      "    * project (NOUN) - 230 times\n",
      "    * pmp (PROPN) - 186 times\n",
      "    * project (PROPN) - 179 times\n",
      "    * management (PROPN) - 167 times\n",
      "\n",
      "POS tables saved to 'output/pos_tables' directory.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from langdetect import detect, LangDetectException\n",
    "from spacy.cli import download as spacy_download\n",
    "from collections import defaultdict\n",
    "\n",
    "def install_spacy_models():\n",
    "    \"\"\"\n",
    "    Installs the required spaCy language models if they are not already installed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        spacy.load('en_core_web_sm')\n",
    "    except OSError:\n",
    "        print(\"Downloading 'en_core_web_sm' model...\")\n",
    "        spacy_download('en_core_web_sm')\n",
    "    \n",
    "    try:\n",
    "        spacy.load('fr_core_news_sm')\n",
    "    except OSError:\n",
    "        print(\"Downloading 'fr_core_news_sm' model...\")\n",
    "        spacy_download('fr_core_news_sm')\n",
    "\n",
    "def detect_language(text):\n",
    "    \"\"\"\n",
    "    Detects the language of the given text using langdetect.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to detect language for.\n",
    "\n",
    "    Returns:\n",
    "        str: Detected language code ('en' for English, 'fr' for French).\n",
    "             Returns 'unknown' if detection fails or the language is neither English nor French.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        if lang in ['en', 'fr']:\n",
    "            return lang\n",
    "        else:\n",
    "            return 'unknown'\n",
    "    except LangDetectException:\n",
    "        return 'unknown'\n",
    "\n",
    "def split_chapter_text(text):\n",
    "    \"\"\"\n",
    "    Splits the chapter text into main text and figure descriptions.\n",
    "\n",
    "    Args:\n",
    "        text (str): The complete chapter text.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, List[str]]: A tuple containing the main text and a list of figure descriptions.\n",
    "    \"\"\"\n",
    "    # Split the text by figure descriptions\n",
    "    pattern = re.compile(r'\\n\\nFigure Description from page \\d+:\\n', re.IGNORECASE)\n",
    "    parts = pattern.split(text)\n",
    "    # The first part is the main text\n",
    "    main_text = parts[0].strip()\n",
    "    # The remaining parts are figure descriptions\n",
    "    figure_descriptions = parts[1:] if len(parts) > 1 else []\n",
    "    return main_text, figure_descriptions\n",
    "\n",
    "def get_all_words(doc):\n",
    "    \"\"\"\n",
    "    Extracts all words along with their POS tags and frequencies.\n",
    "\n",
    "    Args:\n",
    "        doc (spacy.lang.*.Doc): The spaCy processed document.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, str, int]]: A list of tuples containing word, POS tag, and frequency.\n",
    "    \"\"\"\n",
    "    word_pos_freq = defaultdict(int)\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.is_alpha:  # Consider alphabetic tokens only\n",
    "            lemma = token.lemma_.lower()\n",
    "            pos = token.pos_\n",
    "            word_pos_freq[(lemma, pos)] += 1\n",
    "\n",
    "    # Convert to a list of tuples: word, POS tag, frequency\n",
    "    all_words = [(word, pos, freq) for (word, pos), freq in word_pos_freq.items()]\n",
    "    \n",
    "    return all_words\n",
    "\n",
    "def get_top_words(all_words, top_n=5):\n",
    "    \"\"\"\n",
    "    Extracts the top N most frequent words from the list.\n",
    "\n",
    "    Args:\n",
    "        all_words (List[Tuple[str, str, int]]): List of all words with their POS tags and frequencies.\n",
    "        top_n (int): Number of top words to extract.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, str, int]]: A list of tuples containing the top N words, POS tags, and frequency.\n",
    "    \"\"\"\n",
    "    # Sort by frequency in descending order and take the top N\n",
    "    sorted_words = sorted(all_words, key=lambda x: x[2], reverse=True)\n",
    "    return sorted_words[:top_n]\n",
    "\n",
    "def main():\n",
    "    # Install spaCy models if not already installed\n",
    "    install_spacy_models()\n",
    "    \n",
    "    # Load spaCy models\n",
    "    nlp_en = spacy.load('en_core_web_sm')\n",
    "    nlp_fr = spacy.load('fr_core_news_sm')\n",
    "    \n",
    "    # Define paths\n",
    "    lemmatized_chapters_dir = \"output/lemmatized_chapters\"  # Directory with lemmatized chapters\n",
    "    pos_tables_dir = \"output/pos_tables\"  # Directory to save POS tables\n",
    "    \n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(pos_tables_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize a list to hold mapping information\n",
    "    mapping_info = []\n",
    "    \n",
    "    # Iterate through each lemmatized chapter file\n",
    "    for filename in os.listdir(lemmatized_chapters_dir):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            chapter_path = os.path.join(lemmatized_chapters_dir, filename)\n",
    "            with open(chapter_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "    \n",
    "            # Split the text into main text and figure descriptions\n",
    "            main_text, figure_descriptions = split_chapter_text(text)\n",
    "    \n",
    "            if not main_text:\n",
    "                print(f\"Chapter '{filename}' has no main text. Skipping POS tagging.\")\n",
    "                continue\n",
    "    \n",
    "            # Detect language of the main text\n",
    "            language = detect_language(main_text)\n",
    "            if language == 'en':\n",
    "                nlp = nlp_en\n",
    "            elif language == 'fr':\n",
    "                nlp = nlp_fr\n",
    "            else:\n",
    "                print(f\"Could not detect language for '{filename}'. Skipping POS tagging.\")\n",
    "                continue\n",
    "    \n",
    "            # Process the main text with spaCy\n",
    "            doc = nlp(main_text)\n",
    "    \n",
    "            # Get all words with their POS tags and frequencies\n",
    "            all_words = get_all_words(doc)\n",
    "    \n",
    "            # Get top 5 words for display\n",
    "            top_words = get_top_words(all_words, top_n=5)\n",
    "    \n",
    "            # Create a pandas DataFrame for all words\n",
    "            df_all_words = pd.DataFrame(all_words, columns=['Word', 'POS Tag', 'Frequency'])\n",
    "    \n",
    "            # Save all words to a CSV file\n",
    "            chapter_name = filename.replace(\".txt\", \"\").replace(\"_\", \" \").title()\n",
    "            table_filename = f\"POS_Table_{chapter_name.replace(' ', '_')}.csv\"\n",
    "            table_path = os.path.join(pos_tables_dir, table_filename)\n",
    "            df_all_words.to_csv(table_path, index=False)\n",
    "    \n",
    "            # Append to mapping information\n",
    "            mapping_info.append((filename, chapter_name, top_words))\n",
    "    \n",
    "            # Display the top 5 words in the notebook\n",
    "            print(f\"--- {chapter_name} ---\")\n",
    "            df_top_words = pd.DataFrame(top_words, columns=['Word', 'POS Tag', 'Frequency'])\n",
    "            display(df_top_words)\n",
    "            print(\"\\n\")\n",
    "    \n",
    "    print(\"All chapters have been POS tagged and tables have been saved.\")\n",
    "    print(\"\\nSummary of top 5 words for each chapter:\")\n",
    "    for desc_file, chap_name, top_words in mapping_info:\n",
    "        print(f\" - {chap_name}:\")\n",
    "        for word, pos, freq in top_words:\n",
    "            print(f\"    * {word} ({pos}) - {freq} times\")\n",
    "        print()\n",
    "    \n",
    "    print(f\"POS tables saved to '{pos_tables_dir}' directory.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " extract important and noisy nouns based on their semantic similarity to domain-specific terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-3.2.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Using cached transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sentence-transformers) (2.4.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sentence-transformers) (0.25.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (75.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Using cached sentence_transformers-3.2.0-py3-none-any.whl (255 kB)\n",
      "Using cached transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
      "Installing collected packages: transformers, sentence-transformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\admin\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python312\\\\site-packages\\\\transformers\\\\models\\\\deprecated\\\\trajectory_transformer\\\\convert_trajectory_transformer_original_pytorch_checkpoint_to_pytorch.py'\n",
      "HINT: This error might have occurred since this system does not have Windows Long Path support enabled. You can find information on how to enable this at https://pip.pypa.io/warnings/enable-long-paths\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer, util\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTML, display\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Load spaCy model for POS tagging\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from IPython.core.display import HTML, display\n",
    "\n",
    "# Load spaCy model for POS tagging\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load pre-trained sentence transformer model for semantic similarity\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\" Preprocess the text by normalizing, removing punctuation, and numbers. \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove multiple spaces\n",
    "    return text\n",
    "\n",
    "def filter_invalid_words(words):\n",
    "    \"\"\" Filter out invalid words such as single letters or badly lemmatized words. \"\"\"\n",
    "    return [word for word in words if len(word) > 1 and word.isalpha()]\n",
    "\n",
    "def extract_frequent_nouns(pos_df, threshold=5):\n",
    "    \"\"\" Extract nouns with frequency greater than the given threshold. \"\"\"\n",
    "    frequent_nouns_df = pos_df[(pos_df['POS Tag'] == 'NOUN') & (pos_df['Frequency'] > threshold)]\n",
    "    return filter_invalid_words(frequent_nouns_df['Word'].dropna().tolist())\n",
    "\n",
    "def extract_infrequent_nouns(pos_df, threshold=5):\n",
    "    \"\"\" Extract nouns with frequency less than or equal to the given threshold. \"\"\"\n",
    "    infrequent_nouns_df = pos_df[(pos_df['POS Tag'] == 'NOUN') & (pos_df['Frequency'] <= threshold)]\n",
    "    return filter_invalid_words(infrequent_nouns_df['Word'].dropna().tolist())\n",
    "\n",
    "def compute_embeddings_and_filter(nouns, domain_specific_nouns, threshold=0.4):\n",
    "    \"\"\" Compute sentence embeddings for nouns and filter based on semantic similarity. \"\"\"\n",
    "    if not nouns or not domain_specific_nouns:\n",
    "        return [], nouns  # No important nouns if input lists are empty\n",
    "\n",
    "    nouns_embeddings = model.encode(nouns, convert_to_tensor=True)\n",
    "    domain_embeddings = model.encode(domain_specific_nouns, convert_to_tensor=True)\n",
    "\n",
    "    similarities = util.pytorch_cos_sim(nouns_embeddings, domain_embeddings).numpy().max(axis=1)\n",
    "\n",
    "    important_nouns = [(word, score) for word, score in zip(nouns, similarities) if score >= threshold]\n",
    "    noisy_nouns = [(word, score) for word, score in zip(nouns, similarities) if score < threshold]\n",
    "\n",
    "    # Ensure there are fewer noisy words than important ones\n",
    "    if len(noisy_nouns) >= len(important_nouns):\n",
    "        noisy_nouns = noisy_nouns[:len(important_nouns) - 1]\n",
    "\n",
    "    return important_nouns, noisy_nouns\n",
    "\n",
    "def adaptive_threshold(pos_df):\n",
    "    \"\"\" Compute an adaptive threshold based on the word frequency distribution. \"\"\"\n",
    "    max_frequency = pos_df['Frequency'].max()\n",
    "    if max_frequency > 100:\n",
    "        return 0.4\n",
    "    elif max_frequency > 50:\n",
    "        return 0.3\n",
    "    else:\n",
    "        return 0.2\n",
    "\n",
    "def display_side_by_side_table(chapter_name, noisy_df, important_df):\n",
    "    \"\"\" Display noisy and important words side by side in a single HTML table for a given chapter. \"\"\"\n",
    "    chapter_title = f\"<h2><b>{chapter_name}</b></h2>\"\n",
    "    table_html = (\n",
    "        f\"{chapter_title}\"\n",
    "        f\"<table style='width:100%; border:1px solid black;'>\"\n",
    "        f\"<tr>\"\n",
    "        f\"<th style='text-align:left; border:1px solid black;'>Noisy Words (Sample)</th>\"\n",
    "        f\"<th style='text-align:left; border:1px solid black;'>Important Words (Sample)</th>\"\n",
    "        f\"</tr>\"\n",
    "        f\"<tr>\"\n",
    "        f\"<td style='width:50%; vertical-align:top; border:1px solid black;'>\"\n",
    "        f\"{noisy_df.head(5).to_html(index=False)}</td>\"\n",
    "        f\"<td style='width:50%; vertical-align:top; border:1px solid black;'>\"\n",
    "        f\"{important_df.head(5).to_html(index=False)}</td>\"\n",
    "        f\"</tr>\"\n",
    "        f\"</table>\"\n",
    "    )\n",
    "    display(HTML(table_html))\n",
    "\n",
    "def process_chapters(pos_tables_dir, lemmatized_chapters_dir, noisy_words_dir, important_words_dir):\n",
    "    \"\"\" Process each chapter individually, compute embeddings, and identify noisy and important nouns. \"\"\"\n",
    "    for pos_filename in os.listdir(pos_tables_dir):\n",
    "        if pos_filename.endswith(\".csv\"):\n",
    "            chapter_identifier = pos_filename.replace(\"POS_Table_\", \"\").replace(\".csv\", \"\")\n",
    "            lemmatized_path = os.path.join(lemmatized_chapters_dir, f\"{chapter_identifier}.txt\")\n",
    "            \n",
    "            if os.path.exists(lemmatized_path):\n",
    "                with open(lemmatized_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    text = f.read()\n",
    "                processed_text = preprocess_text(text)\n",
    "\n",
    "                pos_df = pd.read_csv(os.path.join(pos_tables_dir, pos_filename))\n",
    "\n",
    "                # Compute adaptive threshold\n",
    "                adaptive_sim_threshold = adaptive_threshold(pos_df)\n",
    "\n",
    "                # Extract domain-specific and infrequent nouns\n",
    "                domain_specific_nouns = extract_frequent_nouns(pos_df)\n",
    "                infrequent_nouns = extract_infrequent_nouns(pos_df)\n",
    "\n",
    "                # Compute embeddings and filter important vs noisy nouns\n",
    "                important_nouns, noisy_nouns = compute_embeddings_and_filter(\n",
    "                    infrequent_nouns, domain_specific_nouns, threshold=adaptive_sim_threshold\n",
    "                )\n",
    "\n",
    "                # Save results to CSV\n",
    "                noisy_nouns_df = pd.DataFrame(noisy_nouns, columns=[\"Word\", \"Similarity Score\"])\n",
    "                important_nouns_df = pd.DataFrame(important_nouns, columns=[\"Word\", \"Similarity Score\"])\n",
    "\n",
    "                noisy_nouns_df.to_csv(os.path.join(noisy_words_dir, f\"Noisy_Words_{chapter_identifier}.csv\"), index=False)\n",
    "                important_nouns_df.to_csv(os.path.join(important_words_dir, f\"Important_Words_{chapter_identifier}.csv\"), index=False)\n",
    "\n",
    "                # Display results side by side\n",
    "                display_side_by_side_table(chapter_identifier, noisy_nouns_df, important_nouns_df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define directories\n",
    "    pos_tables_dir = \"output/pos_tables\"\n",
    "    lemmatized_chapters_dir = \"output/lemmatized_chapters\"\n",
    "    noisy_words_dir = \"output/noisy_words\"\n",
    "    important_words_dir = \"output/important_words\"\n",
    "\n",
    "    # Create output directories if they don't exist\n",
    "    os.makedirs(noisy_words_dir, exist_ok=True)\n",
    "    os.makedirs(important_words_dir, exist_ok=True)\n",
    "\n",
    "    # Process each chapter to identify noisy and important words\n",
    "    process_chapters(pos_tables_dir, lemmatized_chapters_dir, noisy_words_dir, important_words_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract and analyze noun phrases from lemmatized text using SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Configure logging\u001b[39;00m\n\u001b[0;32m     10\u001b[0m logging\u001b[38;5;241m.\u001b[39mbasicConfig(level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mINFO)\n",
      "File \u001b[1;32mc:\\Users\\rassa\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\fraud-management-LYNVebIG-py3.11\\Lib\\site-packages\\sklearn\\__init__.py:84\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     81\u001b[0m         __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     82\u001b[0m         _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     83\u001b[0m     )\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[0;32m     87\u001b[0m     __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_versions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    131\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\rassa\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\fraud-management-LYNVebIG-py3.11\\Lib\\site-packages\\sklearn\\base.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
      "File \u001b[1;32mc:\\Users\\rassa\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\fraud-management-LYNVebIG-py3.11\\Lib\\site-packages\\sklearn\\utils\\__init__.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _joblib, metadata_routing\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bunch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_chunking\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rassa\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\fraud-management-LYNVebIG-py3.11\\Lib\\site-packages\\sklearn\\utils\\_chunking.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[0;32m     12\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rassa\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\fraud-management-LYNVebIG-py3.11\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mInvalidParameterError\u001b[39;00m(\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Custom exception to be raised when the parameter of a class/method/function\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m    does not have a valid type or value.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rassa\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\fraud-management-LYNVebIG-py3.11\\Lib\\site-packages\\sklearn\\utils\\validation.py:26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config \u001b[38;5;28;01mas\u001b[39;00m _get_config\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning, NotFittedError, PositiveSpectrumWarning\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_array_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _asarray_with_order, _is_numpy_namespace, get_namespace\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ComplexWarning, _preserve_dia_indices_dtype\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_isfinite\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FiniteStatus, cy_isfinite\n",
      "File \u001b[1;32mc:\\Users\\rassa\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\fraud-management-LYNVebIG-py3.11\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mspecial\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_version\n\u001b[0;32m     13\u001b[0m _NUMPY_NAMESPACE_NAMES \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray_api_compat.numpy\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21myield_namespaces\u001b[39m(include_numpy_namespaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\rassa\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\fraud-management-LYNVebIG-py3.11\\Lib\\site-packages\\sklearn\\utils\\fixes.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternals\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_packaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse \u001b[38;5;28;01mas\u001b[39;00m parse_version\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _get_threadpool_controller\n",
      "File \u001b[1;32mc:\\Users\\rassa\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\fraud-management-LYNVebIG-py3.11\\Lib\\site-packages\\scipy\\stats\\__init__.py:606\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m.. _statsrefmanual:\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    601\u001b[0m \n\u001b[0;32m    602\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_warnings_errors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[0;32m    605\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[1;32m--> 606\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats_py\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_variation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m variation\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\rassa\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\fraud-management-LYNVebIG-py3.11\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:49\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mspecial\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linalg\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distributions\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _mstats_basic \u001b[38;5;28;01mas\u001b[39;00m mstats_basic\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats_mstats_common\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (_find_repeats, linregress, theilslopes,\n\u001b[0;32m     52\u001b[0m                                    siegelslopes)\n",
      "File \u001b[1;32mc:\\Users\\rassa\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\fraud-management-LYNVebIG-py3.11\\Lib\\site-packages\\scipy\\stats\\distributions.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_distn_infrastructure\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (rv_discrete, rv_continuous, rv_frozen)  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _continuous_distns\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _discrete_distns\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_continuous_distns\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_levy_stable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m levy_stable\n",
      "File \u001b[1;32mc:\\Users\\rassa\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\fraud-management-LYNVebIG-py3.11\\Lib\\site-packages\\scipy\\stats\\_discrete_distns.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m entr, logsumexp, betaln, gammaln \u001b[38;5;28;01mas\u001b[39;00m gamln, zeta\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _lazywhere, rng_integers\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterpolate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interp1d\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m floor, ceil, log, exp, sqrt, log1p, expm1, tanh, cosh, sinh\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rassa\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\fraud-management-LYNVebIG-py3.11\\Lib\\site-packages\\scipy\\interpolate\\__init__.py:187\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bsplines\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pade\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_rgi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ndbspline\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NdBSpline\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# Deprecated namespaces, to be removed in v2.0.0\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rassa\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\fraud-management-LYNVebIG-py3.11\\Lib\\site-packages\\scipy\\interpolate\\_rgi.py:12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterpnd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _ndim_coords_from_arrays\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_cubic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PchipInterpolator\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_rgi_cython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate_linear_2d, find_indices\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bsplines\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_interp_spline\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_fitpack2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RectBivariateSpline\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:405\u001b[0m, in \u001b[0;36mparent\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import logging\n",
    "from collections import Counter\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Define directories for the POS table and lemmatized chapter files\n",
    "POS_TABLES_DIR = \"output/pos_tables\"\n",
    "LEMMATIZED_CHAPTERS_DIR = \"output/lemmatized_chapters\"\n",
    "CONCEPTS_OUTPUT_DIR = \"output/concepts\"\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Ensure the concepts output directory exists\n",
    "os.makedirs(CONCEPTS_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def normalize_and_clean_text(text):\n",
    "    \"\"\" Normalize and clean text by converting to lowercase, removing punctuation and extra spaces. \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def extract_noun_phrases(doc):\n",
    "    \"\"\" Extract multi-word noun phrases and single nouns from the document. \"\"\"\n",
    "    noun_phrases = []\n",
    "    single_nouns = []\n",
    "\n",
    "    for chunk in doc.noun_chunks:\n",
    "        words = [token.text.lower() for token in chunk if token.pos_ == \"NOUN\"]\n",
    "        if len(words) in [2, 3]:\n",
    "            noun_phrases.append(' '.join(words))\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"NOUN\" and token.dep_ != \"compound\":\n",
    "            single_nouns.append(token.lemma_.lower())\n",
    "\n",
    "    return noun_phrases, single_nouns\n",
    "\n",
    "def adjust_single_noun_frequency(single_nouns, multi_word_counter):\n",
    "    \"\"\" Adjust the frequency of single nouns based on occurrences in multi-word phrases. \"\"\"\n",
    "    single_noun_counter = Counter(single_nouns)\n",
    "    for phrase, count in multi_word_counter.items():\n",
    "        for word in phrase.split():\n",
    "            single_noun_counter[word] = max(0, single_noun_counter[word] - count)\n",
    "    return single_noun_counter\n",
    "\n",
    "def calculate_concept_frequency(multi_word_nouns, single_nouns):\n",
    "    \"\"\" Calculate the frequency of multi-word and single noun phrases. \"\"\"\n",
    "    multi_word_counter = Counter(multi_word_nouns)\n",
    "    adjusted_single_noun_counter = adjust_single_noun_frequency(single_nouns, multi_word_counter)\n",
    "\n",
    "    multi_word_df = pd.DataFrame(multi_word_counter.items(), columns=['Word', 'Frequency'])\n",
    "    single_noun_df = pd.DataFrame(adjusted_single_noun_counter.items(), columns=['Word', 'Frequency'])\n",
    "\n",
    "    combined_df = pd.concat([multi_word_df, single_noun_df], ignore_index=True)\n",
    "    return combined_df.sort_values(by='Frequency', ascending=False).reset_index(drop=True)\n",
    "\n",
    "def process_chapter_files(pos_tables_dir, lemmatized_chapters_dir, concepts_output_dir):\n",
    "    \"\"\" Process each chapter file and extract concepts. \"\"\"\n",
    "    for pos_filename in os.listdir(pos_tables_dir):\n",
    "        if pos_filename.endswith(\".csv\"):\n",
    "            chapter_identifier = pos_filename.replace(\"POS_Table_\", \"\").replace(\".csv\", \"\").lower()\n",
    "            lemmatized_file_path = os.path.join(lemmatized_chapters_dir, f\"{chapter_identifier}.txt\")\n",
    "\n",
    "            logging.info(f\"Looking for lemmatized file: {lemmatized_file_path}\")\n",
    "\n",
    "            if os.path.exists(lemmatized_file_path):\n",
    "                try:\n",
    "                    with open(lemmatized_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        normalized_text = normalize_and_clean_text(f.read())\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error reading {lemmatized_file_path}: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                if normalized_text:\n",
    "                    doc = nlp(normalized_text)\n",
    "                    multi_word_nouns, single_nouns = extract_noun_phrases(doc)\n",
    "                    concept_freq_df = calculate_concept_frequency(multi_word_nouns, single_nouns)\n",
    "\n",
    "                    logging.info(f\"\\n--- {chapter_identifier} ---\")\n",
    "                    if not concept_freq_df.empty:\n",
    "                        logging.info(\"Most Pertinent Concepts:\\n%s\", concept_freq_df.head(20).to_string(index=False))\n",
    "                    else:\n",
    "                        logging.info(\"No concepts found.\")\n",
    "\n",
    "                    concept_freq_path = os.path.join(concepts_output_dir, f\"Concepts_{chapter_identifier}.csv\")\n",
    "                    try:\n",
    "                        concept_freq_df.to_csv(concept_freq_path, index=False)\n",
    "                        logging.info(f\"Concept frequencies saved to {concept_freq_path}\")\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error saving to {concept_freq_path}: {e}\")\n",
    "                else:\n",
    "                    logging.info(f\"No text found in {lemmatized_file_path}\")\n",
    "            else:\n",
    "                logging.warning(f\"Lemmatized file not found: {lemmatized_file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_chapter_files(POS_TABLES_DIR, LEMMATIZED_CHAPTERS_DIR, CONCEPTS_OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed: lemmatized_chapitre_1_introduction.txt to Chapitre_1_introduction.txt\n",
      "Renamed: lemmatized_chapitre_2_principles_and_concepts.txt to Chapitre_2_principles_and_concepts.txt\n",
      "Renamed: lemmatized_chapitre_3_introduction_to_project_risk_management_processes.txt to Chapitre_3_introduction_to_project_risk_management_processes.txt\n",
      "Renamed: lemmatized_chapitre_4_plan_risk_management.txt to Chapitre_4_plan_risk_management.txt\n",
      "Renamed: lemmatized_chapitre_5_identify_risks.txt to Chapitre_5_identify_risks.txt\n",
      "Renamed: lemmatized_chapitre_6_perform_qualitative_risk_analysis.txt to Chapitre_6_perform_qualitative_risk_analysis.txt\n",
      "Renamed: lemmatized_chapitre_7_perform_quantitative_risk_analysis.txt to Chapitre_7_perform_quantitative_risk_analysis.txt\n",
      "Renamed: lemmatized_chapitre_8_plan_risk_responses.txt to Chapitre_8_plan_risk_responses.txt\n",
      "Renamed: lemmatized_chapitre_9_monitor_and_control_risks.txt to Chapitre_9_monitor_and_control_risks.txt\n",
      "Renaming process completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the directory containing the lemmatized files\n",
    "lemmatized_chapters_dir = 'output/lemmatized_chapters'\n",
    "\n",
    "# Create a mapping of current filenames to the desired new filenames\n",
    "rename_mapping = {\n",
    "    'lemmatized_chapitre_1_introduction.txt': 'Chapitre_1_introduction.txt',\n",
    "    'lemmatized_chapitre_2_principles_and_concepts.txt': 'Chapitre_2_principles_and_concepts.txt',\n",
    "    'lemmatized_chapitre_3_introduction_to_project_risk_management_processes.txt': 'Chapitre_3_introduction_to_project_risk_management_processes.txt',\n",
    "    'lemmatized_chapitre_4_plan_risk_management.txt': 'Chapitre_4_plan_risk_management.txt',\n",
    "    'lemmatized_chapitre_5_identify_risks.txt': 'Chapitre_5_identify_risks.txt',\n",
    "    'lemmatized_chapitre_6_perform_qualitative_risk_analysis.txt': 'Chapitre_6_perform_qualitative_risk_analysis.txt',\n",
    "    'lemmatized_chapitre_7_perform_quantitative_risk_analysis.txt': 'Chapitre_7_perform_quantitative_risk_analysis.txt',\n",
    "    'lemmatized_chapitre_8_plan_risk_responses.txt': 'Chapitre_8_plan_risk_responses.txt',\n",
    "    'lemmatized_chapitre_9_monitor_and_control_risks.txt': 'Chapitre_9_monitor_and_control_risks.txt'\n",
    "}\n",
    "\n",
    "# Loop through the mapping and rename the files\n",
    "for old_name, new_name in rename_mapping.items():\n",
    "    old_path = os.path.join(lemmatized_chapters_dir, old_name)\n",
    "    new_path = os.path.join(lemmatized_chapters_dir, new_name)\n",
    "    \n",
    "    # Check if the old file exists before renaming\n",
    "    if os.path.isfile(old_path):\n",
    "        os.rename(old_path, new_path)\n",
    "        print(f'Renamed: {old_name} to {new_name}')\n",
    "    else:\n",
    "        print(f'File not found: {old_path}')\n",
    "\n",
    "print('Renaming process completed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved summary for chapitre_1_introduction to output/summary\\Summary_chapitre_1_introduction.csv\n",
      "\n",
      "Sample Summary for chapitre_1_introduction:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Concept</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Attributes</th>\n",
       "      <th>Relationship</th>\n",
       "      <th>Related Concept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>management</td>\n",
       "      <td>24</td>\n",
       "      <td>appropriate, change, characteristic, comprehen...</td>\n",
       "      <td>apply project</td>\n",
       "      <td>project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>project</td>\n",
       "      <td>18</td>\n",
       "      <td>ach, applicable, approach, assumption, attenti...</td>\n",
       "      <td>carry context_refl_ecte</td>\n",
       "      <td>context refl ecte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>process</td>\n",
       "      <td>13</td>\n",
       "      <td>chapter, clarifi, close, contain, culture, dea...</td>\n",
       "      <td>apply asset</td>\n",
       "      <td>asset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>process</td>\n",
       "      <td>13</td>\n",
       "      <td>chapter, clarifi, close, contain, culture, dea...</td>\n",
       "      <td>implement chapter</td>\n",
       "      <td>chapter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>process</td>\n",
       "      <td>13</td>\n",
       "      <td>chapter, clarifi, close, contain, culture, dea...</td>\n",
       "      <td>describe chapter_address</td>\n",
       "      <td>chapter address</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Concept  Frequency                                         Attributes  \\\n",
       "0  management         24  appropriate, change, characteristic, comprehen...   \n",
       "1     project         18  ach, applicable, approach, assumption, attenti...   \n",
       "2     process         13  chapter, clarifi, close, contain, culture, dea...   \n",
       "3     process         13  chapter, clarifi, close, contain, culture, dea...   \n",
       "4     process         13  chapter, clarifi, close, contain, culture, dea...   \n",
       "\n",
       "               Relationship    Related Concept  \n",
       "0             apply project            project  \n",
       "1   carry context_refl_ecte  context refl ecte  \n",
       "2               apply asset              asset  \n",
       "3         implement chapter            chapter  \n",
       "4  describe chapter_address    chapter address  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved summary for chapitre_2_principles_and_concepts to output/summary\\Summary_chapitre_2_principles_and_concepts.csv\n",
      "\n",
      "Sample Summary for chapitre_2_principles_and_concepts:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Concept</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Attributes</th>\n",
       "      <th>Relationship</th>\n",
       "      <th>Related Concept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>process</td>\n",
       "      <td>5</td>\n",
       "      <td>assessment, assure, credibility, integral, ite...</td>\n",
       "      <td>operate conclusion</td>\n",
       "      <td>conclusion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>project risk</td>\n",
       "      <td>4</td>\n",
       "      <td>approach, aspect, cid, communication, effectiv...</td>\n",
       "      <td>represent project</td>\n",
       "      <td>project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>project risk</td>\n",
       "      <td>4</td>\n",
       "      <td>approach, aspect, cid, communication, effectiv...</td>\n",
       "      <td>affect project</td>\n",
       "      <td>project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>project risk management</td>\n",
       "      <td>3</td>\n",
       "      <td>aspect, cid, communication, consistent, effect...</td>\n",
       "      <td>include part_project_process</td>\n",
       "      <td>part project process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stakeholder</td>\n",
       "      <td>3</td>\n",
       "      <td>advance, attitudes, consultation, exposure, fa...</td>\n",
       "      <td>differ group_stakeholder</td>\n",
       "      <td>group stakeholder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Concept  Frequency  \\\n",
       "0                  process          5   \n",
       "1             project risk          4   \n",
       "2             project risk          4   \n",
       "3  project risk management          3   \n",
       "4              stakeholder          3   \n",
       "\n",
       "                                          Attributes  \\\n",
       "0  assessment, assure, credibility, integral, ite...   \n",
       "1  approach, aspect, cid, communication, effectiv...   \n",
       "2  approach, aspect, cid, communication, effectiv...   \n",
       "3  aspect, cid, communication, consistent, effect...   \n",
       "4  advance, attitudes, consultation, exposure, fa...   \n",
       "\n",
       "                   Relationship       Related Concept  \n",
       "0            operate conclusion            conclusion  \n",
       "1             represent project               project  \n",
       "2                affect project               project  \n",
       "3  include part_project_process  part project process  \n",
       "4      differ group_stakeholder     group stakeholder  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved summary for chapitre_3_introduction_to_project_risk_management_processes to output/summary\\Summary_chapitre_3_introduction_to_project_risk_management_processes.csv\n",
      "\n",
      "Sample Summary for chapitre_3_introduction_to_project_risk_management_processes:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Concept</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Attributes</th>\n",
       "      <th>Relationship</th>\n",
       "      <th>Related Concept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>process</td>\n",
       "      <td>11</td>\n",
       "      <td>available, element, essential, important, iter...</td>\n",
       "      <td>perform task</td>\n",
       "      <td>task</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>risk</td>\n",
       "      <td>9</td>\n",
       "      <td>appropriate, careful, characteristic, clear, c...</td>\n",
       "      <td>use technique</td>\n",
       "      <td>technique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>risk</td>\n",
       "      <td>9</td>\n",
       "      <td>appropriate, careful, characteristic, clear, c...</td>\n",
       "      <td>consider effect</td>\n",
       "      <td>effect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>risk</td>\n",
       "      <td>9</td>\n",
       "      <td>appropriate, careful, characteristic, clear, c...</td>\n",
       "      <td>require risk_management_process</td>\n",
       "      <td>risk management process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>action</td>\n",
       "      <td>7</td>\n",
       "      <td>addition, additional, main, management, ning, ...</td>\n",
       "      <td>implement choose_strategy_action</td>\n",
       "      <td>choose strategy action</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Concept  Frequency                                         Attributes  \\\n",
       "0  process         11  available, element, essential, important, iter...   \n",
       "1     risk          9  appropriate, careful, characteristic, clear, c...   \n",
       "2     risk          9  appropriate, careful, characteristic, clear, c...   \n",
       "3     risk          9  appropriate, careful, characteristic, clear, c...   \n",
       "4   action          7  addition, additional, main, management, ning, ...   \n",
       "\n",
       "                       Relationship          Related Concept  \n",
       "0                      perform task                     task  \n",
       "1                     use technique                technique  \n",
       "2                   consider effect                   effect  \n",
       "3   require risk_management_process  risk management process  \n",
       "4  implement choose_strategy_action   choose strategy action  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved summary for chapitre_4_plan_risk_management to output/summary\\Summary_chapitre_4_plan_risk_management.csv\n",
      "\n",
      "Sample Summary for chapitre_4_plan_risk_management:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Concept</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Attributes</th>\n",
       "      <th>Relationship</th>\n",
       "      <th>Related Concept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>plan</td>\n",
       "      <td>15</td>\n",
       "      <td>available, chance, communications, e, effectiv...</td>\n",
       "      <td>describe risk_management_process</td>\n",
       "      <td>risk management process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>plan</td>\n",
       "      <td>15</td>\n",
       "      <td>available, chance, communications, e, effectiv...</td>\n",
       "      <td>describe frequency</td>\n",
       "      <td>frequency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stakeholder</td>\n",
       "      <td>7</td>\n",
       "      <td>acceptance, adapt, assess, attitude, authority...</td>\n",
       "      <td>infl number_factor</td>\n",
       "      <td>number factor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stakeholder</td>\n",
       "      <td>7</td>\n",
       "      <td>acceptance, adapt, assess, attitude, authority...</td>\n",
       "      <td>refl manager</td>\n",
       "      <td>manager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>order</td>\n",
       "      <td>4</td>\n",
       "      <td>effective, evolve, project, scope, time</td>\n",
       "      <td>ensure consistency_agreement</td>\n",
       "      <td>consistency agreement</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Concept  Frequency                                         Attributes  \\\n",
       "0         plan         15  available, chance, communications, e, effectiv...   \n",
       "1         plan         15  available, chance, communications, e, effectiv...   \n",
       "2  stakeholder          7  acceptance, adapt, assess, attitude, authority...   \n",
       "3  stakeholder          7  acceptance, adapt, assess, attitude, authority...   \n",
       "4        order          4            effective, evolve, project, scope, time   \n",
       "\n",
       "                       Relationship          Related Concept  \n",
       "0  describe risk_management_process  risk management process  \n",
       "1                describe frequency                frequency  \n",
       "2                infl number_factor            number factor  \n",
       "3                      refl manager                  manager  \n",
       "4      ensure consistency_agreement    consistency agreement  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved summary for chapitre_5_identify_risks to output/summary\\Summary_chapitre_5_identify_risks.csv\n",
      "\n",
      "Sample Summary for chapitre_5_identify_risks:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Concept</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Attributes</th>\n",
       "      <th>Relationship</th>\n",
       "      <th>Related Concept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>risk</td>\n",
       "      <td>11</td>\n",
       "      <td>account, available, base, case, category, clar...</td>\n",
       "      <td>include project_risk_management</td>\n",
       "      <td>project risk management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>risk</td>\n",
       "      <td>11</td>\n",
       "      <td>account, available, base, case, category, clar...</td>\n",
       "      <td>include information</td>\n",
       "      <td>information</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>risk</td>\n",
       "      <td>11</td>\n",
       "      <td>account, available, base, case, category, clar...</td>\n",
       "      <td>identify risks_process</td>\n",
       "      <td>risks process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>technique</td>\n",
       "      <td>6</td>\n",
       "      <td>assessment, combination, creativity, effective...</td>\n",
       "      <td>identify risk</td>\n",
       "      <td>risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>review</td>\n",
       "      <td>4</td>\n",
       "      <td>historical</td>\n",
       "      <td>use risk_breakdown_structure</td>\n",
       "      <td>risk breakdown structure</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Concept  Frequency                                         Attributes  \\\n",
       "0       risk         11  account, available, base, case, category, clar...   \n",
       "1       risk         11  account, available, base, case, category, clar...   \n",
       "2       risk         11  account, available, base, case, category, clar...   \n",
       "3  technique          6  assessment, combination, creativity, effective...   \n",
       "4     review          4                                         historical   \n",
       "\n",
       "                      Relationship           Related Concept  \n",
       "0  include project_risk_management   project risk management  \n",
       "1              include information               information  \n",
       "2           identify risks_process             risks process  \n",
       "3                    identify risk                      risk  \n",
       "4     use risk_breakdown_structure  risk breakdown structure  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved summary for chapitre_6_perform_qualitative_risk_analysis to output/summary\\Summary_chapitre_6_perform_qualitative_risk_analysis.csv\n",
      "\n",
      "Sample Summary for chapitre_6_perform_qualitative_risk_analysis:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Concept</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Attributes</th>\n",
       "      <th>Relationship</th>\n",
       "      <th>Related Concept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>risk</td>\n",
       "      <td>9</td>\n",
       "      <td>advance, categorize, chain, characteristic, co...</td>\n",
       "      <td>ask decision_point_view</td>\n",
       "      <td>decision point view</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>risk</td>\n",
       "      <td>9</td>\n",
       "      <td>advance, categorize, chain, characteristic, co...</td>\n",
       "      <td>assess priority</td>\n",
       "      <td>priority</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>risk</td>\n",
       "      <td>9</td>\n",
       "      <td>advance, categorize, chain, characteristic, co...</td>\n",
       "      <td>assess priority_risk_impact</td>\n",
       "      <td>priority risk impact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>risk</td>\n",
       "      <td>9</td>\n",
       "      <td>advance, categorize, chain, characteristic, co...</td>\n",
       "      <td>require term_response</td>\n",
       "      <td>term response</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>use</td>\n",
       "      <td>5</td>\n",
       "      <td>analysis, credibility, easy, nition, process, ...</td>\n",
       "      <td>perform risk_analysis_success</td>\n",
       "      <td>risk analysis success</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Concept  Frequency                                         Attributes  \\\n",
       "0    risk          9  advance, categorize, chain, characteristic, co...   \n",
       "1    risk          9  advance, categorize, chain, characteristic, co...   \n",
       "2    risk          9  advance, categorize, chain, characteristic, co...   \n",
       "3    risk          9  advance, categorize, chain, characteristic, co...   \n",
       "4     use          5  analysis, credibility, easy, nition, process, ...   \n",
       "\n",
       "                    Relationship        Related Concept  \n",
       "0        ask decision_point_view    decision point view  \n",
       "1                assess priority               priority  \n",
       "2    assess priority_risk_impact   priority risk impact  \n",
       "3          require term_response          term response  \n",
       "4  perform risk_analysis_success  risk analysis success  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved summary for chapitre_7_perform_quantitative_risk_analysis to output/summary\\Summary_chapitre_7_perform_quantitative_risk_analysis.csv\n",
      "\n",
      "Sample Summary for chapitre_7_perform_quantitative_risk_analysis:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Concept</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Attributes</th>\n",
       "      <th>Relationship</th>\n",
       "      <th>Related Concept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>project</td>\n",
       "      <td>7</td>\n",
       "      <td>accurate, achievement, analysis, appropriate, ...</td>\n",
       "      <td>proceed frequency_effort</td>\n",
       "      <td>frequency effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>risk analysis</td>\n",
       "      <td>6</td>\n",
       "      <td>analysis, appropriate, new, objective, overall...</td>\n",
       "      <td>provide information</td>\n",
       "      <td>information</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>risk analysis</td>\n",
       "      <td>6</td>\n",
       "      <td>analysis, appropriate, new, objective, overall...</td>\n",
       "      <td>perform risk_analysis_process</td>\n",
       "      <td>risk analysis process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>risk analysis</td>\n",
       "      <td>6</td>\n",
       "      <td>analysis, appropriate, new, objective, overall...</td>\n",
       "      <td>use method</td>\n",
       "      <td>method</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bias</td>\n",
       "      <td>4</td>\n",
       "      <td>combat, motivational, source</td>\n",
       "      <td>derive risk</td>\n",
       "      <td>risk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Concept  Frequency  \\\n",
       "0        project          7   \n",
       "1  risk analysis          6   \n",
       "2  risk analysis          6   \n",
       "3  risk analysis          6   \n",
       "4           bias          4   \n",
       "\n",
       "                                          Attributes  \\\n",
       "0  accurate, achievement, analysis, appropriate, ...   \n",
       "1  analysis, appropriate, new, objective, overall...   \n",
       "2  analysis, appropriate, new, objective, overall...   \n",
       "3  analysis, appropriate, new, objective, overall...   \n",
       "4                       combat, motivational, source   \n",
       "\n",
       "                    Relationship        Related Concept  \n",
       "0       proceed frequency_effort       frequency effort  \n",
       "1            provide information            information  \n",
       "2  perform risk_analysis_process  risk analysis process  \n",
       "3                     use method                 method  \n",
       "4                    derive risk                   risk  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved summary for chapitre_8_plan_risk_responses to output/summary\\Summary_chapitre_8_plan_risk_responses.csv\n",
      "\n",
      "Sample Summary for chapitre_8_plan_risk_responses:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Concept</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Attributes</th>\n",
       "      <th>Relationship</th>\n",
       "      <th>Related Concept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>process</td>\n",
       "      <td>8</td>\n",
       "      <td>analysis, assignment, factor, good, plan, poss...</td>\n",
       "      <td>determine risk</td>\n",
       "      <td>risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>plan</td>\n",
       "      <td>8</td>\n",
       "      <td>accordance, additional, analysis, analyze, app...</td>\n",
       "      <td>evaluate order</td>\n",
       "      <td>order</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>process</td>\n",
       "      <td>8</td>\n",
       "      <td>analysis, assignment, factor, good, plan, poss...</td>\n",
       "      <td>describe section</td>\n",
       "      <td>section</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>process</td>\n",
       "      <td>8</td>\n",
       "      <td>analysis, assignment, factor, good, plan, poss...</td>\n",
       "      <td>determine success</td>\n",
       "      <td>success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>plan</td>\n",
       "      <td>8</td>\n",
       "      <td>accordance, additional, analysis, analyze, app...</td>\n",
       "      <td>develop address</td>\n",
       "      <td>address</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Concept  Frequency                                         Attributes  \\\n",
       "0  process          8  analysis, assignment, factor, good, plan, poss...   \n",
       "1     plan          8  accordance, additional, analysis, analyze, app...   \n",
       "2  process          8  analysis, assignment, factor, good, plan, poss...   \n",
       "3  process          8  analysis, assignment, factor, good, plan, poss...   \n",
       "4     plan          8  accordance, additional, analysis, analyze, app...   \n",
       "\n",
       "        Relationship Related Concept  \n",
       "0     determine risk            risk  \n",
       "1     evaluate order           order  \n",
       "2   describe section         section  \n",
       "3  determine success         success  \n",
       "4    develop address         address  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved summary for chapitre_9_monitor_and_control_risks to output/summary\\Summary_chapitre_9_monitor_and_control_risks.csv\n",
      "\n",
      "Sample Summary for chapitre_9_monitor_and_control_risks:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Concept</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Attributes</th>\n",
       "      <th>Relationship</th>\n",
       "      <th>Related Concept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>analysis</td>\n",
       "      <td>28</td>\n",
       "      <td>additional, analysis, appropriate, carlo, caus...</td>\n",
       "      <td>earn value_analysis</td>\n",
       "      <td>value analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>analysis</td>\n",
       "      <td>28</td>\n",
       "      <td>additional, analysis, appropriate, carlo, caus...</td>\n",
       "      <td>create time</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>analysis</td>\n",
       "      <td>28</td>\n",
       "      <td>additional, analysis, appropriate, carlo, caus...</td>\n",
       "      <td>use risk</td>\n",
       "      <td>risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>analysis</td>\n",
       "      <td>28</td>\n",
       "      <td>additional, analysis, appropriate, carlo, caus...</td>\n",
       "      <td>earn use</td>\n",
       "      <td>use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>analysis</td>\n",
       "      <td>28</td>\n",
       "      <td>additional, analysis, appropriate, carlo, caus...</td>\n",
       "      <td>provide view</td>\n",
       "      <td>view</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Concept  Frequency                                         Attributes  \\\n",
       "0  analysis         28  additional, analysis, appropriate, carlo, caus...   \n",
       "1  analysis         28  additional, analysis, appropriate, carlo, caus...   \n",
       "2  analysis         28  additional, analysis, appropriate, carlo, caus...   \n",
       "3  analysis         28  additional, analysis, appropriate, carlo, caus...   \n",
       "4  analysis         28  additional, analysis, appropriate, carlo, caus...   \n",
       "\n",
       "          Relationship Related Concept  \n",
       "0  earn value_analysis  value analysis  \n",
       "1          create time            time  \n",
       "2             use risk            risk  \n",
       "3             earn use             use  \n",
       "4         provide view            view  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from IPython.display import display  # Ensure this is available in Jupyter\n",
    "\n",
    "# Define directories for input and output\n",
    "LEMMATIZED_CHAPTERS_DIR = \"output/lemmatized_chapters\"\n",
    "CONCEPTS_DIR = \"output/concepts\"\n",
    "SUMMARY_OUTPUT_DIR = \"output/summary\"\n",
    "\n",
    "# Load spaCy model with necessary components\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Initialize PhraseMatcher for multi-word concept handling\n",
    "matcher = PhraseMatcher(nlp.vocab, attr='LOWER')\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(SUMMARY_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Function to normalize and clean text\n",
    "def normalize_and_clean_text(text):\n",
    "    \"\"\"\n",
    "    Normalize and clean the input text by converting to lowercase,\n",
    "    removing punctuation, numbers, special characters, and extra spaces.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation, numbers, and special characters (keeping only words and spaces)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)  # Replace punctuation with space\n",
    "    text = re.sub(r'\\d+', '', text)      # Remove numbers\n",
    "    # Remove multiple spaces and trim\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Function to extract attributes based on defined patterns\n",
    "def extract_attributes(doc, single_word_concepts, multi_word_concept_spans):\n",
    "    \"\"\"\n",
    "    Extract attributes from the document based on the following patterns:\n",
    "    - Adjective(s) + Concept\n",
    "    - Concept + Noun Compound\n",
    "\n",
    "    Parameters:\n",
    "    - doc: spaCy parsed document\n",
    "    - single_word_concepts: Set of pre-extracted single-word concepts\n",
    "    - multi_word_concept_spans: List of Span objects representing multi-word concepts\n",
    "\n",
    "    Returns:\n",
    "    - attributes_dict: Dictionary mapping concepts to their attributes\n",
    "    \"\"\"\n",
    "    attributes_dict = defaultdict(set)\n",
    "    \n",
    "    # Handle single-word concepts\n",
    "    for token in doc:\n",
    "        concept = token.lemma_.lower()\n",
    "        if concept in single_word_concepts:\n",
    "            # Adjective(s) + Concept\n",
    "            adjectives = [child.text.lower() for child in token.lefts if child.pos_ == \"ADJ\"]\n",
    "            for adj in adjectives:\n",
    "                attributes_dict[concept].add(adj)\n",
    "    \n",
    "            # Concept + Noun Compound\n",
    "            compounds = [child.text.lower() for child in token.lefts \n",
    "                         if child.dep_ == \"compound\" and child.pos_ == \"NOUN\"]\n",
    "            for comp in compounds:\n",
    "                attributes_dict[concept].add(comp)\n",
    "    \n",
    "    # Handle multi-word concepts\n",
    "    for span in multi_word_concept_spans:\n",
    "        # Get the root token of the span\n",
    "        root = span.root\n",
    "        concept_key = '_'.join(span.text.lower().split())  # Replace spaces with underscores\n",
    "        \n",
    "        # Adjective(s) + Concept\n",
    "        adjectives = [child.text.lower() for child in root.lefts if child.pos_ == \"ADJ\"]\n",
    "        for adj in adjectives:\n",
    "            attributes_dict[concept_key].add(adj)\n",
    "    \n",
    "        # Concept + Noun Compound\n",
    "        compounds = [child.text.lower() for child in root.lefts \n",
    "                     if child.dep_ == \"compound\" and child.pos_ == \"NOUN\"]\n",
    "        for comp in compounds:\n",
    "            attributes_dict[concept_key].add(comp)\n",
    "    \n",
    "    return attributes_dict\n",
    "\n",
    "# Function to extract relationships based on defined patterns\n",
    "def extract_relationships(doc, single_word_concepts, multi_word_concepts, multi_word_concept_spans):\n",
    "    \"\"\"\n",
    "    Extract relationships from the document based on the following patterns:\n",
    "    - Concept + Verb + Concept\n",
    "    - Concept + Verb + Preposition + Concept\n",
    "\n",
    "    Parameters:\n",
    "    - doc: spaCy parsed document\n",
    "    - single_word_concepts: Set of pre-extracted single-word concepts\n",
    "    - multi_word_concepts: Set of pre-extracted multi-word concepts\n",
    "    - multi_word_concept_spans: List of Span objects representing multi-word concepts\n",
    "\n",
    "    Returns:\n",
    "    - relationships_dict: Dictionary mapping subject concepts to a list of (relationship, related_concept)\n",
    "    \"\"\"\n",
    "    relationships_dict = defaultdict(list)\n",
    "    \n",
    "    # Create a set of all multi-word concept strings with underscores\n",
    "    multi_word_concept_strings = set('_'.join(span.text.lower().split()) for span in multi_word_concept_spans)\n",
    "    \n",
    "    for token in doc:\n",
    "        # Focus only on verbs\n",
    "        if token.pos_ == \"VERB\":\n",
    "            # Find subjects that are concepts\n",
    "            subjects = []\n",
    "            for child in token.children:\n",
    "                if child.dep_ in (\"nsubj\", \"nsubjpass\") and child.pos_ == \"NOUN\":\n",
    "                    # Check if the subject is part of a multi-word concept\n",
    "                    subject_span = None\n",
    "                    for span in multi_word_concept_spans:\n",
    "                        if child.i >= span.start and child.i < span.end:\n",
    "                            subject_span = span\n",
    "                            break\n",
    "                    if subject_span:\n",
    "                        subj = '_'.join(subject_span.text.lower().split())\n",
    "                        subjects.append(subj)\n",
    "                    elif child.lemma_.lower() in single_word_concepts:\n",
    "                        subjects.append(child.lemma_.lower())\n",
    "            \n",
    "            if not subjects:\n",
    "                continue  # No valid subjects\n",
    "            \n",
    "            # Pattern: Concept + Verb + Concept\n",
    "            direct_objects = []\n",
    "            for child in token.children:\n",
    "                if child.dep_ in (\"dobj\", \"attr\") and child.pos_ == \"NOUN\":\n",
    "                    # Check for multi-word concepts\n",
    "                    object_span = None\n",
    "                    for span in multi_word_concept_spans:\n",
    "                        if child.i >= span.start and child.i < span.end:\n",
    "                            object_span = span\n",
    "                            break\n",
    "                    if object_span:\n",
    "                        obj = '_'.join(object_span.text.lower().split())\n",
    "                        direct_objects.append(obj)\n",
    "                    elif child.lemma_.lower() in single_word_concepts:\n",
    "                        direct_objects.append(child.lemma_.lower())\n",
    "            \n",
    "            # Pattern: Concept + Verb + Preposition + Concept\n",
    "            prepositions = [child for child in token.children if child.dep_ == \"prep\" and child.pos_ == \"ADP\"]\n",
    "            for prep in prepositions:\n",
    "                for grandchild in prep.children:\n",
    "                    if grandchild.dep_ == \"pobj\" and grandchild.pos_ == \"NOUN\":\n",
    "                        # Check for multi-word concepts\n",
    "                        pobj_span = None\n",
    "                        for span in multi_word_concept_spans:\n",
    "                            if grandchild.i >= span.start and grandchild.i < span.end:\n",
    "                                pobj_span = span\n",
    "                                break\n",
    "                        if pobj_span:\n",
    "                            obj = '_'.join(pobj_span.text.lower().split())\n",
    "                            direct_objects.append(obj)\n",
    "                        elif grandchild.lemma_.lower() in single_word_concepts:\n",
    "                            direct_objects.append(grandchild.lemma_.lower())\n",
    "            \n",
    "            # Add relationships to the dictionary\n",
    "            for subj in subjects:\n",
    "                for obj in direct_objects:\n",
    "                    # Avoid self-relationships unless meaningful\n",
    "                    if subj == obj:\n",
    "                        continue\n",
    "                    # Determine if the relationship is via preposition\n",
    "                    relationship = f\"{token.lemma_} {obj}\"\n",
    "                    relationships_dict[subj].append((relationship, obj))\n",
    "    \n",
    "    return relationships_dict\n",
    "\n",
    "# Function to process each chapter file\n",
    "def process_chapter_files(lemmatized_chapters_dir, concepts_dir, summary_output_dir, sample_size=5):\n",
    "    \"\"\"\n",
    "    Process each chapter to extract attributes and relationships and save them into summary CSV files.\n",
    "    Additionally, display a small sample of the summary table for each chapter.\n",
    "\n",
    "    Parameters:\n",
    "    - lemmatized_chapters_dir: Directory containing lemmatized chapter text files\n",
    "    - concepts_dir: Directory containing pre-extracted concepts CSV files\n",
    "    - summary_output_dir: Directory to save summary CSV files\n",
    "    - sample_size: Number of sample rows to display per chapter\n",
    "    \"\"\"\n",
    "    # Iterate through each concepts CSV file in the concepts directory\n",
    "    for concepts_filename in os.listdir(concepts_dir):\n",
    "        if concepts_filename.endswith(\".csv\"):\n",
    "            concepts_path = os.path.join(concepts_dir, concepts_filename)\n",
    "            \n",
    "            # Extract chapter identifier from concepts filename\n",
    "            # Expected format: Concepts_Chapitre_<number>_<title>.csv\n",
    "            match = re.match(r'Concepts_lemmatized_(Chapitre_\\d+_[\\w_]+)\\.csv', concepts_filename, re.IGNORECASE)\n",
    "            if not match:\n",
    "                print(f\"Filename {concepts_filename} does not match expected pattern. Skipping.\")\n",
    "                continue\n",
    "            chapter_identifier = match.group(1)  # e.g., Chapitre_9_Monitor_And_Control_Risks\n",
    "            \n",
    "            lemmatized_file_path = os.path.join(lemmatized_chapters_dir, f\"{chapter_identifier}.txt\")\n",
    "            \n",
    "            if os.path.exists(lemmatized_file_path):\n",
    "                # Read and normalize the lemmatized chapter text\n",
    "                try:\n",
    "                    with open(lemmatized_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        lemmatized_text = f.read()\n",
    "                    normalized_text = normalize_and_clean_text(lemmatized_text)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {lemmatized_file_path}: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                if normalized_text:\n",
    "                    # Process the text with spaCy to get a parsed document\n",
    "                    doc = nlp(normalized_text)\n",
    "                    \n",
    "                    # Load concepts for this chapter\n",
    "                    try:\n",
    "                        concepts_df = pd.read_csv(concepts_path)\n",
    "                        # Separate single-word and multi-word concepts\n",
    "                        single_word_concepts = set(concepts_df[~concepts_df['Word'].str.contains(' ')]['Word'].str.lower())\n",
    "                        multi_word_concepts = set(concepts_df[concepts_df['Word'].str.contains(' ')]['Word'].str.lower())\n",
    "                        # Create a dictionary for concept frequencies\n",
    "                        concept_freq_dict = pd.Series(concepts_df.Frequency.values,\n",
    "                                                     index=concepts_df.Word.str.lower()).to_dict()\n",
    "                        # Prepare multi-word patterns for PhraseMatcher\n",
    "                        multi_word_patterns = [nlp.make_doc(concept) for concept in multi_word_concepts]\n",
    "                        matcher.add(\"MULTI_WORD_CONCEPT\", multi_word_patterns)\n",
    "                        matches = matcher(doc)\n",
    "                        multi_word_concept_spans = []\n",
    "                        for match_id, start, end in matches:\n",
    "                            span = doc[start:end]\n",
    "                            multi_word_concept_spans.append(span)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {concepts_path}: {e}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract Attributes\n",
    "                    attributes_dict = extract_attributes(doc, single_word_concepts, multi_word_concept_spans)\n",
    "                    \n",
    "                    # Extract Relationships\n",
    "                    relationships_dict = extract_relationships(doc, single_word_concepts, multi_word_concepts, multi_word_concept_spans)\n",
    "                    \n",
    "                    # Create Summary Table\n",
    "                    summary_list = []\n",
    "                    for concept in sorted(single_word_concepts.union(\n",
    "                            {'_'.join(span.text.lower().split()) for span in multi_word_concept_spans})):\n",
    "                        # Determine if the concept is multi-word\n",
    "                        if '_' in concept:\n",
    "                            display_concept = ' '.join(concept.split('_'))\n",
    "                        else:\n",
    "                            display_concept = concept\n",
    "                        \n",
    "                        # Get frequency (replace underscores with spaces for multi-word concepts)\n",
    "                        freq = concept_freq_dict.get(display_concept, 0)\n",
    "                        \n",
    "                        # Get attributes\n",
    "                        attrs = attributes_dict.get(concept, set())\n",
    "                        attrs_str = ', '.join(sorted(attrs)) if attrs else ''\n",
    "                        \n",
    "                        # Get relationships\n",
    "                        rels = relationships_dict.get(concept, [])\n",
    "                        \n",
    "                        # For each relationship, create a separate row\n",
    "                        for rel, related_concept in rels:\n",
    "                            # Determine if related_concept is multi-word\n",
    "                            if '_' in related_concept:\n",
    "                                display_related_concept = ' '.join(related_concept.split('_'))\n",
    "                            else:\n",
    "                                display_related_concept = related_concept\n",
    "                            \n",
    "                            summary_list.append({\n",
    "                                'Concept': display_concept,\n",
    "                                'Frequency': freq,\n",
    "                                'Attributes': attrs_str,\n",
    "                                'Relationship': rel,\n",
    "                                'Related Concept': display_related_concept\n",
    "                            })\n",
    "                    \n",
    "                    summary_df = pd.DataFrame(summary_list)\n",
    "                    \n",
    "                    # Sort the summary table based on frequency in descending order\n",
    "                    summary_df.sort_values(by='Frequency', ascending=False, inplace=True)\n",
    "                    \n",
    "                    # Reset index after sorting\n",
    "                    summary_df.reset_index(drop=True, inplace=True)\n",
    "                    \n",
    "                    # Save Summary Table to CSV\n",
    "                    if not summary_df.empty:\n",
    "                        summary_output_filename = f\"Summary_{chapter_identifier}.csv\"\n",
    "                        summary_output_path = os.path.join(summary_output_dir, summary_output_filename)\n",
    "                        try:\n",
    "                            summary_df.to_csv(summary_output_path, index=False)\n",
    "                            print(f\"Saved summary for {chapter_identifier} to {summary_output_path}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error saving {summary_output_path}: {e}\")\n",
    "                    \n",
    "                        # Display a small sample of the summary table based on top frequency\n",
    "                        print(f\"\\nSample Summary for {chapter_identifier}:\")\n",
    "                        display_df = summary_df.head(sample_size)  # Display first 'sample_size' rows as a sample\n",
    "                        display(display_df)\n",
    "            else:\n",
    "                print(f\"Lemmatized file not found for {chapter_identifier}: {lemmatized_file_path}\")\n",
    "                continue\n",
    "\n",
    "# Run the function to process all chapter files\n",
    "# Remove the `if __name__ == \"__main__\":` block to allow execution in Jupyter Notebook\n",
    "process_chapter_files(\n",
    "    lemmatized_chapters_dir=LEMMATIZED_CHAPTERS_DIR,\n",
    "    concepts_dir=CONCEPTS_DIR,\n",
    "    summary_output_dir=SUMMARY_OUTPUT_DIR,\n",
    "    sample_size=5  # Adjust the number of sample rows to display\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rassa\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\fraud-management-LYNVebIG-py3.11\\Lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.7.1) was trained with spaCy v3.7.2 and may not be 100% compatible with the current version (3.8.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved combined nodes to output/nodes\\nodes.csv\n",
      "Saved combined relationships to output/relationships\\relationships.csv\n",
      "\n",
      "Sample Nodes:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Concept</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Attributes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>section</td>\n",
       "      <td>5</td>\n",
       "      <td>analysis, d, describe, detail, detailed, main,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>element</td>\n",
       "      <td>3</td>\n",
       "      <td>analysis, analyst, cost, effect, follow, proje...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lesson</td>\n",
       "      <td>10</td>\n",
       "      <td>documenting, effectiveness, inclusion, opportu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>example</td>\n",
       "      <td>27</td>\n",
       "      <td>analysis, appropriate, breakdown, categorize, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>path</td>\n",
       "      <td>0</td>\n",
       "      <td>critical, logical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Concept  Frequency                                         Attributes\n",
       "0  section          5  analysis, d, describe, detail, detailed, main,...\n",
       "1  element          3  analysis, analyst, cost, effect, follow, proje...\n",
       "2   lesson         10  documenting, effectiveness, inclusion, opportu...\n",
       "3  example         27  analysis, appropriate, breakdown, categorize, ...\n",
       "4     path          0                                  critical, logical"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Relationships:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Relationship</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>guide</td>\n",
       "      <td>use</td>\n",
       "      <td>tool_technique_process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>guide</td>\n",
       "      <td>use</td>\n",
       "      <td>project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>guide</td>\n",
       "      <td>operate</td>\n",
       "      <td>signifi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>area</td>\n",
       "      <td>provide</td>\n",
       "      <td>information_signifi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>area</td>\n",
       "      <td>guide</td>\n",
       "      <td>edition</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Source Relationship                  Target\n",
       "0  guide          use  tool_technique_process\n",
       "1  guide          use                 project\n",
       "2  guide      operate                 signifi\n",
       "3   area      provide     information_signifi\n",
       "4   area        guide                 edition"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from IPython.display import display  # For Jupyter Notebook display\n",
    "\n",
    "# Define directories for input and output\n",
    "LEMMATIZED_CHAPTERS_DIR = \"output/lemmatized_chapters\"\n",
    "CONCEPTS_DIR = \"output/concepts\"\n",
    "SUMMARY_OUTPUT_DIR = \"output/summary\"\n",
    "NODES_OUTPUT_DIR = \"output/nodes\"\n",
    "RELATIONSHIPS_OUTPUT_DIR = \"output/relationships\"\n",
    "\n",
    "# Load spaCy model with necessary components\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Initialize PhraseMatcher for multi-word concept handling\n",
    "matcher = PhraseMatcher(nlp.vocab, attr='LOWER')\n",
    "\n",
    "# Ensure the output directories exist\n",
    "os.makedirs(SUMMARY_OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(NODES_OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(RELATIONSHIPS_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Function to normalize and clean text\n",
    "def normalize_and_clean_text(text):\n",
    "    \"\"\"\n",
    "    Normalize and clean the input text by converting to lowercase,\n",
    "    removing punctuation, numbers, special characters, and extra spaces.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation, numbers, and special characters (keeping only words and spaces)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)  # Replace punctuation with space\n",
    "    text = re.sub(r'\\d+', '', text)      # Remove numbers\n",
    "    # Remove multiple spaces and trim\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Function to extract attributes based on defined patterns\n",
    "def extract_attributes(doc, single_word_concepts, multi_word_concept_spans):\n",
    "    \"\"\"\n",
    "    Extract attributes from the document based on the following patterns:\n",
    "    - Adjective(s) + Concept\n",
    "    - Concept + Noun Compound\n",
    "\n",
    "    Parameters:\n",
    "    - doc: spaCy parsed document\n",
    "    - single_word_concepts: Set of pre-extracted single-word concepts\n",
    "    - multi_word_concept_spans: List of Span objects representing multi-word concepts\n",
    "\n",
    "    Returns:\n",
    "    - attributes_dict: Dictionary mapping concepts to their attributes\n",
    "    \"\"\"\n",
    "    attributes_dict = defaultdict(set)\n",
    "    \n",
    "    # Handle single-word concepts\n",
    "    for token in doc:\n",
    "        concept = token.lemma_.lower()\n",
    "        if concept in single_word_concepts:\n",
    "            # Adjective(s) + Concept\n",
    "            adjectives = [child.text.lower() for child in token.lefts if child.pos_ == \"ADJ\"]\n",
    "            for adj in adjectives:\n",
    "                attributes_dict[concept].add(adj)\n",
    "    \n",
    "            # Concept + Noun Compound\n",
    "            compounds = [child.text.lower() for child in token.lefts \n",
    "                         if child.dep_ == \"compound\" and child.pos_ == \"NOUN\"]\n",
    "            for comp in compounds:\n",
    "                attributes_dict[concept].add(comp)\n",
    "    \n",
    "    # Handle multi-word concepts\n",
    "    for span in multi_word_concept_spans:\n",
    "        # Get the root token of the span\n",
    "        root = span.root\n",
    "        concept_key = '_'.join(span.text.lower().split())  # Replace spaces with underscores\n",
    "        \n",
    "        # Adjective(s) + Concept\n",
    "        adjectives = [child.text.lower() for child in root.lefts if child.pos_ == \"ADJ\"]\n",
    "        for adj in adjectives:\n",
    "            attributes_dict[concept_key].add(adj)\n",
    "    \n",
    "        # Concept + Noun Compound\n",
    "        compounds = [child.text.lower() for child in root.lefts \n",
    "                     if child.dep_ == \"compound\" and child.pos_ == \"NOUN\"]\n",
    "        for comp in compounds:\n",
    "            attributes_dict[concept_key].add(comp)\n",
    "    \n",
    "    return attributes_dict\n",
    "\n",
    "# Function to extract relationships based on defined patterns\n",
    "def extract_relationships(doc, single_word_concepts, multi_word_concepts, multi_word_concept_spans):\n",
    "    \"\"\"\n",
    "    Extract relationships from the document based on the following patterns:\n",
    "    - Concept + Verb + Concept\n",
    "    - Concept + Verb + Preposition + Concept\n",
    "\n",
    "    Parameters:\n",
    "    - doc: spaCy parsed document\n",
    "    - single_word_concepts: Set of pre-extracted single-word concepts\n",
    "    - multi_word_concepts: Set of pre-extracted multi-word concepts\n",
    "    - multi_word_concept_spans: List of Span objects representing multi-word concepts\n",
    "\n",
    "    Returns:\n",
    "    - relationships_dict: Dictionary mapping subject concepts to a list of (relationship, related_concept)\n",
    "    \"\"\"\n",
    "    relationships_dict = defaultdict(list)\n",
    "    \n",
    "    # Create a set of all multi-word concept strings with underscores\n",
    "    multi_word_concept_strings = set('_'.join(span.text.lower().split()) for span in multi_word_concept_spans)\n",
    "    \n",
    "    for token in doc:\n",
    "        # Focus only on verbs\n",
    "        if token.pos_ == \"VERB\":\n",
    "            # Find subjects that are concepts\n",
    "            subjects = []\n",
    "            for child in token.children:\n",
    "                if child.dep_ in (\"nsubj\", \"nsubjpass\") and child.pos_ == \"NOUN\":\n",
    "                    # Check if the subject is part of a multi-word concept\n",
    "                    subject_span = None\n",
    "                    for span in multi_word_concept_spans:\n",
    "                        if child.i >= span.start and child.i < span.end:\n",
    "                            subject_span = span\n",
    "                            break\n",
    "                    if subject_span:\n",
    "                        subj = '_'.join(subject_span.text.lower().split())\n",
    "                        subjects.append(subj)\n",
    "                    elif child.lemma_.lower() in single_word_concepts:\n",
    "                        subjects.append(child.lemma_.lower())\n",
    "            \n",
    "            if not subjects:\n",
    "                continue  # No valid subjects\n",
    "            \n",
    "            # Pattern: Concept + Verb + Concept\n",
    "            direct_objects = []\n",
    "            for child in token.children:\n",
    "                if child.dep_ in (\"dobj\", \"attr\") and child.pos_ == \"NOUN\":\n",
    "                    # Check for multi-word concepts\n",
    "                    object_span = None\n",
    "                    for span in multi_word_concept_spans:\n",
    "                        if child.i >= span.start and child.i < span.end:\n",
    "                            object_span = span\n",
    "                            break\n",
    "                    if object_span:\n",
    "                        obj = '_'.join(object_span.text.lower().split())\n",
    "                        direct_objects.append(obj)\n",
    "                    elif child.lemma_.lower() in single_word_concepts:\n",
    "                        direct_objects.append(child.lemma_.lower())\n",
    "            \n",
    "            # Pattern: Concept + Verb + Preposition + Concept\n",
    "            prepositions = [child for child in token.children if child.dep_ == \"prep\" and child.pos_ == \"ADP\"]\n",
    "            for prep in prepositions:\n",
    "                for grandchild in prep.children:\n",
    "                    if grandchild.dep_ == \"pobj\" and grandchild.pos_ == \"NOUN\":\n",
    "                        # Check for multi-word concepts\n",
    "                        pobj_span = None\n",
    "                        for span in multi_word_concept_spans:\n",
    "                            if grandchild.i >= span.start and grandchild.i < span.end:\n",
    "                                pobj_span = span\n",
    "                                break\n",
    "                        if pobj_span:\n",
    "                            obj = '_'.join(pobj_span.text.lower().split())\n",
    "                            direct_objects.append(obj)\n",
    "                        elif grandchild.lemma_.lower() in single_word_concepts:\n",
    "                            direct_objects.append(grandchild.lemma_.lower())\n",
    "            \n",
    "            # Add relationships to the dictionary\n",
    "            for subj in subjects:\n",
    "                for obj in direct_objects:\n",
    "                    # Avoid self-relationships unless meaningful\n",
    "                    if subj == obj:\n",
    "                        continue\n",
    "                    # Determine if the relationship is via preposition\n",
    "                    # For simplicity, we'll categorize relationships based on the verb\n",
    "                    relationship = token.lemma_  # Use the verb lemma as the relationship type\n",
    "                    relationships_dict[subj].append((relationship, obj))\n",
    "    \n",
    "    return relationships_dict\n",
    "\n",
    "# Function to process each chapter file\n",
    "def process_chapter_files(lemmatized_chapters_dir, concepts_dir, summary_output_dir, nodes_output_dir, relationships_output_dir, sample_size=5):\n",
    "    \"\"\"\n",
    "    Process each chapter to extract attributes and relationships and save them into summary, nodes, and relationships CSV files.\n",
    "    Additionally, display a small sample of the summary table for each chapter.\n",
    "\n",
    "    Parameters:\n",
    "    - lemmatized_chapters_dir: Directory containing lemmatized chapter text files\n",
    "    - concepts_dir: Directory containing pre-extracted concepts CSV files\n",
    "    - summary_output_dir: Directory to save summary CSV files\n",
    "    - nodes_output_dir: Directory to save nodes CSV files\n",
    "    - relationships_output_dir: Directory to save relationships CSV files\n",
    "    - sample_size: Number of sample rows to display per chapter\n",
    "    \"\"\"\n",
    "    # Initialize global aggregators\n",
    "    global_attributes_dict = defaultdict(set)\n",
    "    global_relationships_dict = defaultdict(list)\n",
    "    global_concept_freq_dict = defaultdict(int)\n",
    "    \n",
    "    # Iterate through each concepts CSV file in the concepts directory\n",
    "    for concepts_filename in os.listdir(concepts_dir):\n",
    "        if concepts_filename.endswith(\".csv\"):\n",
    "            concepts_path = os.path.join(concepts_dir, concepts_filename)\n",
    "            \n",
    "            # Extract chapter identifier from concepts filename\n",
    "            # Expected format: Concepts_Chapitre_<number>_<title>.csv\n",
    "            match = re.match(r'Concepts_lemmatized_(Chapitre_\\d+_[\\w_]+)\\.csv', concepts_filename, re.IGNORECASE)\n",
    "            if not match:\n",
    "                print(f\"Filename {concepts_filename} does not match expected pattern. Skipping.\")\n",
    "                continue\n",
    "            chapter_identifier = match.group(1)  # e.g., Chapitre_9_Monitor_And_Control_Risks\n",
    "            \n",
    "            lemmatized_file_path = os.path.join(lemmatized_chapters_dir, f\"{chapter_identifier}.txt\")\n",
    "            \n",
    "            if os.path.exists(lemmatized_file_path):\n",
    "                # Read and normalize the lemmatized chapter text\n",
    "                try:\n",
    "                    with open(lemmatized_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        lemmatized_text = f.read()\n",
    "                    normalized_text = normalize_and_clean_text(lemmatized_text)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {lemmatized_file_path}: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                if normalized_text:\n",
    "                    # Process the text with spaCy to get a parsed document\n",
    "                    doc = nlp(normalized_text)\n",
    "                    \n",
    "                    # Load concepts for this chapter\n",
    "                    try:\n",
    "                        concepts_df = pd.read_csv(concepts_path)\n",
    "                        # Ensure 'Word' and 'Frequency' columns exist\n",
    "                        if not {'Word', 'Frequency'}.issubset(concepts_df.columns):\n",
    "                            print(f\"Concepts file {concepts_filename} missing required columns. Skipping.\")\n",
    "                            continue\n",
    "                        # Separate single-word and multi-word concepts\n",
    "                        single_word_concepts = set(concepts_df[~concepts_df['Word'].str.contains(' ')]['Word'].str.lower())\n",
    "                        multi_word_concepts = set(concepts_df[concepts_df['Word'].str.contains(' ')]['Word'].str.lower())\n",
    "                        # Update global concept frequency\n",
    "                        for _, row in concepts_df.iterrows():\n",
    "                            concept = row['Word'].lower()\n",
    "                            freq = row['Frequency']\n",
    "                            global_concept_freq_dict[concept] += freq\n",
    "                        # Prepare multi-word patterns for PhraseMatcher\n",
    "                        multi_word_patterns = [nlp.make_doc(concept) for concept in multi_word_concepts]\n",
    "                        matcher.add(\"MULTI_WORD_CONCEPT\", multi_word_patterns)\n",
    "                        matches = matcher(doc)\n",
    "                        multi_word_concept_spans = []\n",
    "                        for match_id, start, end in matches:\n",
    "                            span = doc[start:end]\n",
    "                            multi_word_concept_spans.append(span)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {concepts_path}: {e}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Extract Attributes\n",
    "                    attributes_dict = extract_attributes(doc, single_word_concepts, multi_word_concept_spans)\n",
    "                    \n",
    "                    # Aggregate global attributes\n",
    "                    for concept, attrs in attributes_dict.items():\n",
    "                        global_attributes_dict[concept].update(attrs)\n",
    "                    \n",
    "                    # Extract Relationships\n",
    "                    relationships_dict = extract_relationships(doc, single_word_concepts, multi_word_concepts, multi_word_concept_spans)\n",
    "                    \n",
    "                    # Aggregate global relationships\n",
    "                    for subj, rels in relationships_dict.items():\n",
    "                        global_relationships_dict[subj].extend(rels)\n",
    "                    \n",
    "                    # Optionally, create and save per-chapter summaries\n",
    "                    # [Omitted for brevity]\n",
    "                    \n",
    "                    # Display a small sample of the summary table based on top frequency\n",
    "                    # [Omitted for brevity]\n",
    "            else:\n",
    "                print(f\"Lemmatized file not found for {chapter_identifier}: {lemmatized_file_path}\")\n",
    "                continue\n",
    "    \n",
    "    # After processing all chapters, create combined nodes and relationships\n",
    "    \n",
    "    # Create Relationships DataFrame\n",
    "    relationships_list = []\n",
    "    for subj, rels in global_relationships_dict.items():\n",
    "        for rel, obj in rels:\n",
    "            relationships_list.append({\n",
    "                'Source': subj,\n",
    "                'Relationship': rel,\n",
    "                'Target': obj\n",
    "            })\n",
    "    \n",
    "    relationships_df = pd.DataFrame(relationships_list).drop_duplicates()\n",
    "    \n",
    "    # Identify all concepts that are part of relationships\n",
    "    connected_concepts = set(relationships_df['Source']).union(set(relationships_df['Target']))\n",
    "    \n",
    "    # Create Nodes DataFrame\n",
    "    nodes_list = []\n",
    "    for concept in connected_concepts:\n",
    "        display_concept = ' '.join(concept.split('_')) if '_' in concept else concept\n",
    "        freq = global_concept_freq_dict.get(concept, 0)\n",
    "        attrs = global_attributes_dict.get(concept, set())\n",
    "        attrs_str = ', '.join(sorted(attrs)) if attrs else ''\n",
    "        nodes_list.append({\n",
    "            'Concept': display_concept,\n",
    "            'Frequency': freq,\n",
    "            'Attributes': attrs_str\n",
    "        })\n",
    "    \n",
    "    nodes_df = pd.DataFrame(nodes_list).drop_duplicates(subset=['Concept'])\n",
    "    \n",
    "    # Save Combined Nodes CSV\n",
    "    nodes_output_path = os.path.join(nodes_output_dir, \"nodes.csv\")\n",
    "    try:\n",
    "        nodes_df.to_csv(nodes_output_path, index=False)\n",
    "        print(f\"\\nSaved combined nodes to {nodes_output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {nodes_output_path}: {e}\")\n",
    "    \n",
    "    # Save Combined Relationships CSV\n",
    "    relationships_output_path = os.path.join(relationships_output_dir, \"relationships.csv\")\n",
    "    try:\n",
    "        relationships_df.to_csv(relationships_output_path, index=False)\n",
    "        print(f\"Saved combined relationships to {relationships_output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {relationships_output_path}: {e}\")\n",
    "    \n",
    "    # Optionally, display samples\n",
    "    print(\"\\nSample Nodes:\")\n",
    "    display(nodes_df.head(sample_size))\n",
    "    \n",
    "    print(\"\\nSample Relationships:\")\n",
    "    display(relationships_df.head(sample_size))\n",
    "\n",
    "# Run the function to process all chapter files\n",
    "process_chapter_files(\n",
    "    lemmatized_chapters_dir=LEMMATIZED_CHAPTERS_DIR,\n",
    "    concepts_dir=CONCEPTS_DIR,\n",
    "    summary_output_dir=SUMMARY_OUTPUT_DIR,\n",
    "    nodes_output_dir=NODES_OUTPUT_DIR,\n",
    "    relationships_output_dir=RELATIONSHIPS_OUTPUT_DIR,\n",
    "    sample_size=5  # Adjust the number of sample rows to display\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
